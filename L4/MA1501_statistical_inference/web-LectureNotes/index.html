<html>

    <head>
                        <link rel="stylesheet" type="text/css" href="static/css/latex2web.css"/>
        

        	
			
<!-- load mathjax config **before** MathJax.js -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    TeX: {
      Macros: {
        RR: "{\\bf R}",
        pounds: '{\\unicode{xA3}}',
      }
    },
    "HTML-CSS": {
		availableFonts: ["TeX"],
		linebreaks: { 
			automatic: true, 
			width: "container"
		}
	 }
  });
</script>

<!--script src="static/Mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	

<script type="text/javascript" 
    src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js">
</script>

<script type="text/javascript" src="static/js/showhide.js"></script>        
			 
		<title>MA1501: Statistical Inference</title>
	   
     </head>
	
<body>
		<div class="macros">
		</div>
<div id="container" style="min-height:500px;">

	<div id="institution">
			
	</div>
	<div id="breadcrumbs">
			    <a href="index.html">MA1501: Statistical Inference</a>
	</div>
	<div id="columnwrap">
		
		<div id="content-main">
        		<h1 class="booktitle">
	<table>
		<tr><th align="left">Title</th><td>MA1501: Statistical Inference</td></tr>
		<tr><th align="left">Author</th><td>Dr Jonathan Gillard</td></tr>			</table>
	</h1>

															

										<h3 class="subsection">Topics Covered (informal description)</h3>
									<ol class="enumerate">
            					<li class="item">Sampling distributions</li>								<li class="item">Point estimation and confidence intervals</li>								<li class="item">Hypothesis testing</li>								<li class="item">Linear regression (fitting a straight line)</li>			
        </ol>								

										<h3 class="subsection">How will this module be lectured?</h3>
									
Main mode of lecturing will be 'fill in the blanks' on provided lecture notes.  Additional handouts, and material presented via the board/computer may also be given. Electronic lecture notes are not available for this module.

Example lectures will just be used as normal lectures.



										<h3 class="subsection">Assessment</h3>
									
The summative assessment (								$100\%$								) is through an unseen written examination (two hours).

Formative assessments will be given out. Feedback on these formative assessments will be given via (i) written comments, (ii) tutorial sessions (which will go over the assessments) and (iii) videos available on Learning Central which will go through the solutions of some of the problems on the formative assessments (which I will call Example Sheets).

										<h3 class="subsection">Reading List</h3>
									
Probability and Statistical Inference, by Hogg and Tanis.

Many other suitable books - too many to mention (just look in library for any introductory statistics text).






																<div class="section">
    													<div class="subsection">
    													
For the moment, consider a population as a collection of objects, such as people, families, cars etc.  A sample is a sub-collection or part of the population.  Populations are studied because they have some property or characteristic that varies among different members of the population.  Such a characteristic is called a variable e.g. the monthly income of families, the fuel consumption of a car etc.

A variable identifies a property of interest, and is the basis upon which values are associated with members of the population.  Formal definitions are:
																																<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;1&nbsp;		</div>
A <span class="emph">
       population
    </span> is the collection of all values of the variable under study.
</div>								<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;2&nbsp;		</div>
A <span class="emph">
       sample
    </span> is any sub-collection of the population.
</div>								<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;3&nbsp;		</div>
A <span class="emph">
       population parameter
    </span> is some numerical measure associated with the population as a whole.
</div>																																<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;4&nbsp;		</div>
Let $X$ denote a random variable. A <span class="emph">
       random sample from the distribution
    </span> of $X$   is a set of independent and identically distributed (i.i.d) random variables $X_{1},X_{2},\ldots,X_{n}$.
</div>								<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;5&nbsp;		</div>
The values taken by $X_{1},X_{2},\ldots,X_{n}$ in an actual sample are denoted by $x_{1},x_{2},\ldots,x_{n}$ and are called the <span class="emph">
       sample values
    </span>.
</div>								<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;6&nbsp;		</div>
A <span class="emph">
       statistic
    </span> is a function of $X_{1},X_{2},\ldots,X_{n}$  and is thus itself a random variable.  It does not contain any unknown parameters.
</div>								

			
    </div>								<div class="subsection">
    																					

																
We use 								$(n-1)$								  in the denominator compared with the definition of the variance of the population 								$(n)$								.  We use 								$(n-1)$								  instead of 								$n$								 for a practical reason, and shall prove why it is beneficial to use 								$(n-1)$								 later.

To emphasize the occasions when we are treating the sample mean and sample variance as random variables, we shall use the notation 								$\bar{X}$								 and 								$S^{2}$								 respectively.

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;7&nbsp;		</div>$s^2$ may also be written as $\displaystyle s^{2}=\frac{1}{n-1}\left\{\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2}\right\}=\frac{1}{n-1}\left\{\sum_{i=1}^{n}x_{i}^{2}-\frac{\left(\sum_{i=1}^{n}x_{i}\right)^{2}}{n}\right\}\, .$</div>																

			
    </div>								<div class="subsection">
    																					Motivating example								
A purse contains six coins: one 5p, two 10p, one 20p, two 50p coins.  Consider the selection of two coins from the purse, obtained with replacement. Let 								$X$								 be the value of a randomly chosen coin, and let 								$X_{1}$								 and 								$X_{2}$								 be the values of the two selected coins.

We can construct the probability distribution of 								$X$								:
								<div class="center">
       <table class="tabular">				
            					<tr class="t">				
        					<td class="lL">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					:  or 																col1-col2																col3-col4								 ...
								$x$			
	</td>								<td class="cL">				
        					 5p			
	</td>								<td class="cL">				
        					 10p			
	</td>								<td class="cL">				
        					 20p			
	</td>								<td class="cLR">				
        					 50p			
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]								$P[X=x]$			
	</td>								<td class="cL">				
        					&nbsp;			
	</td>								<td class="cL">				
        					&nbsp;			
	</td>								<td class="cL">				
        					&nbsp;			
	</td>								<td class="cLR">				
        					&nbsp;			
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>			
	</tr>			
	</table>
    </div>								
We can also compute 								$E[X]=$								<br/>								
and we can compute 								$Var[X]=$								

Now consider the distribution of 								$(X_{1},X_{2})$								:
								<div class="center">
       <table class="tabular">				
            					<tr class="t">				
        					<td class="lL">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					:  or 																col1-col2																col3-col4								 ...
								$(x_{1},x_{2})$			
	</td>								<td class="cL">				
        					 No. of ways			
	</td>								<td class="cL">				
        					 Prob.			
	</td>								<td class="cL">				
        					$\bar{x}$			
	</td>								<td class="cLR">				
        					$s^{2}$			
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					
  (5p,5p)			
	</td>								<td class="cL">				
        					 1			
	</td>								<td class="cL">				
        					$\frac{1}{36}$			
	</td>								<td class="cL">				
        					 5			
	</td>								<td class="cLR">				
        					 0			
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]
  (5p,10p)			
	</td>								<td class="cL">				
        					 4			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					 7.5			
	</td>								<td class="cLR">				
        					 12.5			
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cL">				
        
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					[8pt]			
	</td>			
	</tr>			
	</table>
    </div>								
We can now work out the sampling distribution for 								$\bar{X}$								:
								<div class="center">
       <table class="tabular">				
            					<tr class="t">				
        					<td class="lL">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					:  or 																col1-col2																col3-col4								 ...
								$\bar{x}$			
	</td>								<td class="cL">				
        					 5			
	</td>								<td class="cL">				
        					 7.5			
	</td>								<td class="cL">				
        					 10			
	</td>								<td class="cL">				
        					 12.5			
	</td>								<td class="cL">				
        					 15			
	</td>								<td class="cL">				
        					 20			
	</td>								<td class="cL">				
        					 27.5			
	</td>								<td class="cL">				
        					 30			
	</td>								<td class="cL">				
        					 35			
	</td>								<td class="cLR">				
        					 50			
	</td>			
	</tr>								<tr class="tb">				
        					<td class="lL">				
        					$P[\bar{X}=\bar{x}]$			
	</td>								<td class="cL">				
        					$\frac{1}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					$\frac{2}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					$\frac{1}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					$\frac{8}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cLR">				
        					$\frac{4}{36}$			
	</td>			
	</tr>			
	</table>
    </div>								
and 								$E[\bar{X}]=24.1667=E[X]$								, 								$\displaystyle Var[\bar{X}]=176.736=\frac{Var[X]}{2}$								.

The sampling distribution for 								$S^{2}$								 is:
								<div class="center">
       <table class="tabular">				
            					<tr class="t">				
        					<td class="lL">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					:  or 																col1-col2																col3-col4								 ...
								$s^{2}$			
	</td>								<td class="cL">				
        					 0			
	</td>								<td class="cL">				
        					 12.5			
	</td>								<td class="cL">				
        					 50			
	</td>								<td class="cL">				
        					 112.5			
	</td>								<td class="cL">				
        					 450			
	</td>								<td class="cL">				
        					 800			
	</td>								<td class="cLR">				
        					 1012.5			
	</td>			
	</tr>								<tr class="tb">				
        					<td class="lL">				
        					$P[S^{2}=s^{2}]$			
	</td>								<td class="cL">				
        					$\frac{10}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					$\frac{2}{36}$			
	</td>								<td class="cL">				
        					$\frac{4}{36}$			
	</td>								<td class="cL">				
        					$\frac{8}{36}$			
	</td>								<td class="cLR">				
        					$\frac{4}{36}$			
	</td>			
	</tr>			
	</table>
    </div>								
and 								$E[S^{2}]=353.4722=Var[X]$								. We can also compute 								$Var[S^2]$								 if we wish. We can also do the same for other sample statistics, such as the sample minimum.

																Sampling from a distribution: commonly used results								<div class="theorem">
		<div class="blockheading"> 
    Theorem&nbsp;8&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{n}$ be a random sample from a distribution with mean $\mu$ and variance $\sigma^2$. Then $\displaystyle E[\bar{X}]=\mu$ and $\displaystyle Var[\bar{X}]=\frac{\sigma^2}{n}.$</div>																

								<div class="theorem">
		<div class="blockheading"> 
    Theorem&nbsp;9&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{n}$ be a random sample from a distribution with mean $\mu$ and variance $\sigma^2$. Then $\displaystyle E[S^{2}]=\sigma^2$.
</div>																

								<div class="theorem">
		<div class="blockheading"> 
    Theorem&nbsp;10&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{n}$ be a random sample from a continuous distribution with cumulative distribution function $F(x)$ and probability density function $f(x).$ The probability density function of the maximum and minimum of $X_{1},X_{2},\ldots,X_{n}$ are given by $\displaystyle g(z)=nf(z)[F(z)]^{n-1}$ and $\displaystyle h(w)=nf(w)[1-F(w)]^{n-1},$ where $z=\max(x_{1},x_{2},\ldots,x_{n})$ and $w=\min(x_{1},x_{2},\ldots,x_{n}).$</div>																
It is also possible to prove a similar result for discrete distributions, but we will not do that in this module.
								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;11&nbsp;		</div>
The random variable $X$ has probability density function
<div class="displaymath">
       \[f(x)=12x^{2}(1-x)\,\,\, 0\leq x \leq 1.\]
    </div> Obtain the probability density function of the sample maximum, when a random sample of size $n$ is taken from $X$. Hence, or otherwise, find the probability that the largest maximum is $\frac{1}{2}.$</div>																

			
    </div>								<div class="subsection">
    													


																Normal distribution								
Let 								$X$								 be a normally distributed random variable. Its probability density function is given by 								<div class="displaymath">
       \[f(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{ -\frac{(x-\mu)^2}{2\sigma^2}\right\}.\]
    </div>								
We have that 								$E[X]=$								<br/>								
and 								$Var[X]=$								<br/>								
For brevity we write 								$X \sim N[\mu,\sigma^2].$								

Graphs of this probability density function for different 								$\mu$								 and 								$\sigma^2$								 are given below:
																																

																																																																								
If 								$X$								 is normally distributed, the so-called standardized random variable 								$\displaystyle Z=\frac{X-\mu}{\sigma}$								 is also normally distributed, with mean 0 and variance 1. Its probabilities and percentiles are tabulated.

																																																


								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;12&nbsp;		</div>
Let $X \sim N[4,3^2]$. Find $P[X<5]$, $P[X>7]$ and the value of $x$ such that<br/>$P[X<x]=0.95$.
</div>																<div class="theorem">
		<div class="blockheading"> 
    Theorem&nbsp;13&nbsp;		</div>
If $X_{1},X_{2},\ldots,X_{n}$ are independent normally distributed random variables, such that each $X_{i}$ has mean $\mu_{i}$ and variance $\sigma_{i}^{2}$, then for any constants $a_{i}$, $i=1,2,\ldots,n$, the random variable $\displaystyle\sum_{i=1}^{n}a_{i}X_{i}$:<br/>
a. is normally distributed,<br/>
b. has mean $\displaystyle\sum_{i=1}^{n}a_{i}\mu_{i}$,<br/>
c. has variance $\displaystyle\sum_{i=1}^{n}a_{i}^{2}\sigma_{i}^{2}$.
</div>								
This tells us that 'a linear combination of normally distributed variables is also normally distributed'.

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;14&nbsp;		</div>
If $X_{1},X_{2},\ldots,X_{n}$ are independent and identically distributed normal random variables, such that each $X_{i}$ has mean $\mu$ and variance $\sigma^{2}$, then $\displaystyle\sum_{i=1}^{n}X_{i}\sim N\left[n\mu, n\sigma^2\right]$.
</div>								

																

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;15&nbsp;		</div>
If $X_{1},X_{2},\ldots,X_{n}$ are independent normally distributed random variables, such that each $X_{i}$ has mean $\mu$ and variance $\sigma^{2}$, then $\displaystyle\bar{X}\sim N\left[\mu, \frac{\sigma^{2}}{n}\right]$.
</div>								

																

																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;16&nbsp;		</div>
The weights of sacks of potatoes are normally distributed with mean $25kg$ and standard deviation $1kg$. Find:<br/>
(i) the probability that the mean weight of a random sample of four sacks is greater than $26kg$;<br/>
(ii) the probability that the total weight of a random sample of 16 sacks lies between $396kg$ and $405kg$;<br/>
(iii) the sample size necessary for the sample mean to be within $0.25kg$ of the true mean $25kg$ at least $95\%$ of the time.
</div>																


																Students' t-distribution								
In probability and statistics, Students' t-distribution (or simply the t-distribution) is a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. The t-distribution is symmetric and bell-shaped, like the normal distribution, but has heavier tails, meaning that it is more prone to producing values that fall far from its mean. Let the random variable 								$X$								 have the Students' t-distribution. Its probability density function is given by:
								<div class="displaymath">
       \[f(x;\nu) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu\pi}\,\Gamma(\frac{\nu}{2})}\left(1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}}.\]
    </div>								
It depends on one parameter 								$\nu$								, called the 								<span class="emph">
       degrees of freedom
    </span>								. For brevity we write 								$X \sim t(\nu)$								.
																																								
Note that as 								$\nu\rightarrow\infty$								 the Students' t-distribution behaves like the normal distribution.

Plots of the probability density function for the Students' t-distribution for different 								$\nu$								 are given below:

																																

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;17&nbsp;		</div>
If $X_{1},X_{2},\ldots,X_{n}$ is a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$, then the random variable $\displaystyle T=\frac{\bar{X}-\mu}{S/\sqrt{n}}$ has the Students' t-distribution with $(n-1)$ degrees of freedom.
</div>								
Most statistical tables give only the percentiles of 								$T$								, for example, with 								$n=11$								, 								$P[T<1.812]=0.95.$								

																Chi-squared distribution								
For 								$n$								 a positive integer, the chi-squared distribution is the distribution of
								<div class="displaymath">
       \[
X_{1}^{2}+X_{2}^{2}+\ldots+X_{n}^{2}\]
    </div>								
where
								$X_{1},X_{2},\ldots,X_{n}$								 are all independently and identically normally distributed with zero mean, and unit variance.

The chi-squared distribution depends on one parameter, which again is called the degrees of freedom.

Shortly, if 								$X$								 is a random variable with chi-squared distribution with 								$\nu$								 we can write 								$X \sim\chi^{2}(\nu)$								. Here 								$E[X]=\nu$								 and 								$Var[X]=2\nu$								.

Note that this distribution is not symmetric. A plot of the chi-squared distribution for different degrees of freedom 								$\nu$								 is given below:

																																



								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;18&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{n}$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. Then $\displaystyle\frac{(n-1)S^{2}}{\sigma^{2}}$ has a chi-squared distribution with $(n-1)$ degrees of freedom.
</div>								


																F-distribution								
The F-distribution is another skewed distribution which depends on two parameters, again known as two degrees of freedom 								$m$								 and 								$n$								. If 								$X \sim\chi^{2}(m)$								 and 								$Y \sim\chi^{2}(n)$								 are two independent random variables, then
								<div class="displaymath">
       \[
Z=\frac{X/m}{Y/n}\sim F(m,n)
\]
    </div>								
is said to have the F distribution with 								$m$								 and 								$n$								 degrees of freedom. A plot of the F distribution with different degrees of freedom  is given below:

																																

Note that if 								$Z \sim F(m,n)$								 then 								$1/Z \sim F(n,m).$								

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;19&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{m}$ and $Y_{1},Y_{2},\ldots,Y_{n}$ be random samples from $X\sim N[\mu_{X},\sigma_{X}^{2}]$ and $Y\sim N[\mu_{Y},\sigma_{Y}^{2}] $ respectively. Let $S_{X}^{2}$ and $S_{Y}^{2}$ be the corresponding sample variances. It follows that $\displaystyle\frac{(m-1)S_{X}^{2}}{\sigma_{X}^{2}}\sim\chi^{2}(m-1)$ and $\displaystyle\frac{(n-1)S_{Y}^{2}}{\sigma_{Y}^{2}}\sim\chi^{2}(n-1).$
Hence <div class="displaymath">
       \[\frac{S_{X}^{2}/\sigma_{X}^{2}}{S_{Y}^{2}/\sigma_{Y}^{2}}\sim F(m-1,n-1).\]
    </div></div>																

																																																																																																								


																The Central Limit Theorem								

								<div class="theorem">
		<div class="blockheading"> 
    Theorem&nbsp;20&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{n}$ be independent, identically distributed random variables with mean $\mu$ and variance $\sigma^{2}$. Then:<br/>
a. $\displaystyle\sum_{i=1}^{n}X_{i}\sim N\left[n\mu, n\sigma^2\right]$, approximately<br/>
b. $\displaystyle\bar{X}\sim N\left[\mu, \frac{\sigma^{2}}{n}\right]$,
approximately.<br/> The approximation improves as $n \rightarrow\infty.$</div>								

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;21&nbsp;		</div>
The number of typing errors made on a page follows a Poisson distribution with mean 2. Use the central limit theorem to calculate (approximately) the probability that there are more than 950 typing errors in a 450 page book.
</div>																

											
    </div>			
    </div>								<div class="section">
    													<div class="subsection">
    													
In statistics, we take a sample which we use to infer things regarding a population. Thats why this module is called 'Statistical Inference'.

Statistical inference is needed to answer questions such as:
								<ul class="itemize">
            					<li class="item">What are the voting intentions before an election? 								<span class="emph">
       Market research, opinion
polls, surveys
    </span></li>								<li class="item">What is the effect of obesity on life expectancy? 								<span class="emph">
       Epidemiology
    </span></li>								<li class="item">What is the average benefit of a new cancer therapy? 								<span class="emph">
       Clinical trials
    </span></li>								<li class="item">What proportion of temperature change is due to man? 								<span class="emph">
       Environmental
statistics
    </span></li>								<li class="item">What is the benefit of speed cameras? 								<span class="emph">
       Traffic studies
    </span></li>								<li class="item">What portfolio maximises expected return? 								<span class="emph">
       Financial and actuarial
applications
    </span></li>								<li class="item">How confident are we the Higgs Boson exists? 								<span class="emph">
       Science
    </span></li>								<li class="item">What are possible benefits and harms of genetically-modified plants?
								<span class="emph">
       Agricultural experiments
    </span></li>								<li class="item">What proportion of the UK economy involves prostitution and illegal drugs?
								<span class="emph">
       Official statistics
    </span></li>								<li class="item">What is the chance Liverpool will best Arsenal next week? 								<span class="emph">
       Sport
    </span></li>			
        </ul>								

In the population we have population parameters, properties of the population that we may or may not know. Upon taking a sample, we can compute sample statistics. For example, to estimate the population mean, we may take a sample, then compute a sample mean, and this seems a reasonable approach.

However, sometimes, there may be several alternative estimators that can be used to estimate a parameter. We therefore need criteria to compare estimators and to decide which is the 'best' in a particular situation.

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;22&nbsp;		</div>
Let $X$ be the random variable denoting the duration between calls to 999. $X$ is known to follow an exponential distribution, with probability density function
<div class="equation">
       \begin{equation}
f(x;\lambda)=\frac{1}{\lambda}\exp(-x/\lambda), \,\,\, x \geq 0 \, .
\end{equation}
    </div>
You are asked to compute the probability that the duration between calls is less than 1 minute. You correctly note that to do this, you need to estimate $\lambda$. Suggest a strategy to estimate $\lambda$.
</div>								

Methods of finding estimators are covered in later modules. Here we concentrate on
								<ol class="enumerate">
            					<li class="item">'Obvious' estimators, or ones that can be deduced from sensible statistical reasoning</li>								<li class="item">Properties of estimators</li>								<li class="item">Potential ways to compare estimators</li>			
        </ol>								

For notation, and to be general, let 								$\theta$								 denote a general population parameter that we wish to estimate. Estimators are often denoted by a circumflex above the parameter e.g. 								$\hat{\theta}$								 is an estimator of 								$\theta$								.
																

																

								<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;23&nbsp;		</div>
The mean square error (MSE) of an estimator $\hat{\theta}$ of $\theta$ is defined as <div class="displaymath">
       \[MSE[\hat{\theta}]=E \left[(\hat{\theta}-\theta)^{2}\right] \,.\]
    </div></div>								

								<div class="theorem">
		<div class="blockheading"> 
    Theorem&nbsp;24&nbsp;		</div>
The mean square error (MSE) of an estimator $\hat{\theta}$ of $\theta$ may be written as <div class="displaymath">
       \[MSE[\hat{\theta}]=Var[\hat{\theta}]+\{bias[\hat{\theta}]\}^2\]
    </div></div>																

If 								$\hat{\theta}_{1}$								 and 								$\hat{\theta}_{2}$								 are two estimators of a parameter 								$\theta$								, 								$\hat{\theta}_{1}$								 is said to be a better estimator, in mean square error, than 								$\hat{\theta}_{2}$								 if 								$MSE[\hat{\theta}_{1}]<MSE[\hat{\theta}_{2}]$								.


								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;25&nbsp;		</div>
A scientist wants to estimate the volume $v$ of a cuboid whose unequal edges are of lengths $x$, $y$ and $z$. The scientist can estimate the volume using either of these two methods.

<span class="emph">
       Method (i): Obtain a direct measurement $V$ of the volume. $V$ can be regarded as a random variable having mean $v$ and variance $\sigma_{V}^{2}$. <br/>
      Method (ii): Obtain measurements $X$, $Y$ and $Z$ of the unequal edges, and estimate the volume using $\hat{v}=XYZ$. It may be assumed that $X$, $Y$ and $Z$ are independent random variables with respective means $x$, $y$ and $z$ and common variance $\sigma^{2}$.
    </span>

Show that method (ii) gives an unbiased estimate of the volume, but that method (i) is preferable to method (ii) if <div class="displaymath">
       \[\sigma_{V}^{2}<\sigma^{6}+\sigma^{4}(x^{2}+y^{2}+z^{2})+\sigma^{2}(x^{2}y^{2}+x^{2}z^{2}+y^{2}z^{2}).\]
    </div></div>								

																



			
    </div>								<div class="subsection">
    													
The discussed properties of estimators provide valuable information on comparing and choosing an estimator, but say rather little about the quality of a particular estimate.
								<div class="definition">
		<div class="blockheading"> 
    Definition&nbsp;26&nbsp;		</div>
 A $100(1-\alpha)\%$ confidence interval for an unknown parameter $\theta$, with estimator $\hat{\theta}$ is an interval
<div class="displaymath">
       \[C=[\hat{\theta}-l,\hat{\theta}+u]\]
    </div>
for a lower value $l$ and upper value $u$ such that
<div class="displaymath">
       \[P[\theta\in C]=1-\alpha\, .\]
    </div></div>								
This interval 								$C$								 is random, because the value of 								$\hat{\theta}$								 depends on the data.  In the long run, 								$100(1 - \alpha)\%$								 of confidence intervals will contain 								$\theta$								.


																Confidence intervals for means, with normally distributed population and variance known								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;27&nbsp;		</div>
Construct a $100(1-\alpha)\%$ confidence interval for the unknown mean $\mu$ of a normally distributed population, with known variance $\sigma^{2}$, having observed a random sample $X_{1},X_{2},\ldots, X_{n}$ from this population.
</div>																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;28&nbsp;		</div>
A sample of size $n=4$ gave values 12.4, 13.6, 12.9 and 13.4 when randomly sampled from a normally distributed population with unknown mean $\mu$, and variance $\sigma^{2}=0.25^2.$ Compute a $95\%$ confidence interval for $\mu.$</div>								

																



								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;29&nbsp;		</div>
Let $X_{1},X_{2},\ldots, X_{m}$ and $Y_{1},Y_{2},\ldots,Y_{n}$ be two random samples, each from a normal distribution with unknown means $\mu_{1}$ and $\mu_{2}$, but known variances $\sigma_{1}^{2}$ and $\sigma_{2}^{2}$ respectively. A $100(1-\alpha)\%$ confidence interval for the difference of the unknown means $\mu_{1}-\mu_{2}$, is given by <div class="displaymath">
       \[(\bar{x}-\bar{y})\pm z_{1-\alpha/2}\sqrt{\frac{\sigma_{1}^{2}}{m}+\frac{\sigma_{2}^{2}}{n}}\, .\]
    </div></div>																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;30&nbsp;		</div>
The mean life of two types of light bulbs, A and B, were compared by testing 20 bulbs of type A and 25 bulbs of type B. The sample means, in obvious notation, were given by $\bar{x}_{A}=1021.3$ hours and $\bar{x}_{B}=1005.7$ hours. Assuming that the life of both types of bulbs is normally distributed with standard deviation 30 hours (for both cases), find a $95\%$ confidence interval for the difference in the population means for both types.
</div>																

																Confidence intervals for means, with normally distributed population, but variance unknown								
In the previous section we considered confidence intervals for means when the population variance was known if we have an independent and identically distributed sample from a normally distributed population. Hence, we use the fact that 								<div class="displaymath">
       \[Z=\frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N[0,1]\]
    </div>								
to construct a probability statement concerning 								$Z$								, and then algebraically manipulate this to achieve a probability statement for 								$\bar{X}$								.

If we do not know what 								$\sigma^{2}$								 is, we can make use of the following fact considered in the previous chapter:
								<div class="quote">
       If 								$X_{1},X_{2},\ldots,X_{n}$								 is a random sample from a normal distribution with mean 								$\mu$								 and variance 								$\sigma^2$								, then the random variable 								<div class="displaymath">
       \[ T=\frac{\bar{X}-\mu}{S/\sqrt{n}}\]
    </div>								 has the Students' t-distribution with 								$(n-1)$								 degrees of freedom.
    </div>								

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;31&nbsp;		</div>
Construct a $100(1-\alpha)\%$ confidence interval for the unknown mean $\mu$ of a normally distributed population, with unknown variance, having observed a random sample $X_{1},X_{2},\ldots, X_{n}.$</div>																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;32&nbsp;		</div>
The heights of 16 randomly selected children aged 11 were measured, with sample mean 121cm. The sample variance was calculated to be 25cm. Assuming that these heights are normally distributed, find the $90\%$ confidence interval for the population mean height.
</div>																

We now wish to find confidence intervals for the difference of two population means. Let 								$X_{1},X_{2},\ldots, X_{m}$								 and 								$Y_{1},Y_{2},\ldots,Y_{n}$								 be two random samples, each from a normal distribution with unknown means 								$\mu_{1}$								 and 								$\mu_{2}$								 and known variances 								$\sigma_{1}^{2}$								 and 								$\sigma_{2}^{2}$								 respectively.

If we knew the variances, then we can use the fact that
								<div class="displaymath">
       \[\bar{X}-\bar{Y}\sim N \left[\mu_{1}-\mu_{2},\frac{\sigma_{1}^{2}}{m}+\frac{\sigma_{2}^{2}}{n}\right]\, ,\]
    </div>								
and derive a confidence interval in the usual way.

If we do not know 								$\sigma_{1}^{2}$								 and 								$\sigma_{2}^{2}$								, we again can use Students' t-distribution, but we also can consider two other options:
								<ol class="enumerate">
            					<li class="item">$\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma^{2}.$								 In this case we assume both samples come from populations which may have equal variances. This means we can use both samples combined to estimate 								$\sigma^2$								, rather than using each sample separately, to consequently estimate 								$\sigma_{1}^{2}$								 and 								$\sigma_{2}^{2}$								 separately.</li>								<li class="item">$\sigma_{1}^{2}\neq\sigma_{2}^{2}.$								 In this case we assume both samples come from populations which do not have equal variances, and so we have to consequently estimate 								$\sigma_{1}^{2}$								 and 								$\sigma_{2}^{2}$								 separately.</li>			
        </ol>																Case 1. 								$\sigma_{1}^{2}=\sigma_{2}^{2}=\sigma^{2}$								
Let 								$S_{X}^{2}$								 be the sample variance computed from 								$X_{1},X_{2},\ldots, X_{m}$								, and let 								$S_{Y}^{2}$								 be the sample variance computed from 								$Y_{1},Y_{2},\ldots,Y_{n}$								. Both of these are estimators of 								$\sigma^{2}$								.
																

One can show that
								<div class="displaymath">
       \[
T=\frac{(\bar{X}-\bar{Y})-(\mu_{1}-\mu_{2})}{S\sqrt{\frac{1}{m}+\frac{1}{n}}}\]
    </div>								
has the Students' t distribution with 								$m+n-2$								 degrees of freedom. Hence one may construct a 								$100(1-\alpha)\%$								 confidence interval for 								$\mu_{1}-\mu_{2}$								 as
								<div class="displaymath">
       \[
(\bar{x}-\bar{y}) \pm t_{1-\alpha/2}(m+n-2)s\sqrt{\frac{1}{m}+\frac{1}{n}}\, .
\]
    </div>								

																Case 2. 								$\sigma_{1}^{2}\neq\sigma_{2}^{2}$								
Let 								$S_{X}^{2}$								 be the sample variance computed from 								$X_{1},X_{2},\ldots, X_{m}$								, and let 								$S_{Y}^{2}$								 be the sample variance computed from 								$Y_{1},Y_{2},\ldots,Y_{n}$								.
If 								$\sigma_{1}^{2}\neq\sigma_{2}^{2}$								 then
								<div class="displaymath">
       \[
T=\frac{(\bar{X}-\bar{Y})-(\mu_{1}-\mu_{2})}{\sqrt{\frac{S_{X}^{2}}{m}+\frac{S_{Y}^{2}}{n}}}\]
    </div>								
can be shown to have a Students' t-distribution with 								$\nu$								 degrees of freedom.

But 								$\nu$								 can only be estimated from
								<div class="displaymath">
       \[\nu=\frac{\left(\frac{s_{X}^{2}}{m}+\frac{s_{Y}^{2}}{n}\right)^2}{\frac{\left(\frac{s_{X}^{2}}{m}\right)^2}{m-1}+\frac{\left(\frac{s_{Y}^{2}}{n}\right)^2}{n-1}}\,.
\]
    </div>								
This is known as Satterthwaite's approximation.

Hence one may construct a 								$100(1-\alpha)\%$								 confidence interval for 								$\mu_{1}-\mu_{2}$								 as
								<div class="displaymath">
       \[
(\bar{x}-\bar{y}) \pm t_{1-\alpha/2}(\nu)\sqrt{\frac{s_{X}^{2}}{m}+\frac{s_{Y}^{2}}{n}}\, .
\]
    </div>								

																Confidence intervals for means, with normally distributed population, with paired observations								
Sometimes the assumption that two random variables 								$X$								 and 								$Y$								 are independent is inappropriate because there is a natural pairing of results. For example, lets imagine that I measure your systolic blood pressure today, and then measure it again tomorrow. You would provide me with two measurements, and it is not appropriate to assume that both measurements are independent.

Suppose however that I still want to obtain a confidence interval for the difference of the population means 								$X$								 and 								$Y$								.
																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;33&nbsp;		</div>
An experiment was conducted to measure mileages of cars under two types of petrol: Fast Oil, and Slicker. Ten cars are initially used with Fast Oil petrol, and these cars are used again under the same conditions but with Slicker petrol. The results were as follows:
<div class="center">
       <table class="tabular">				
            <tr class="t">				
        <td class="rL">				
        
	</td>
	</tr><tr class="t">				
        <td class="rL">				
        :  or col1-col2col3-col4 ...

  Car:
	</td><td class="cL">				
         1
	</td><td class="cL">				
         2
	</td><td class="cL">				
         3
	</td><td class="cL">				
         4
	</td><td class="cL">				
         5
	</td><td class="cL">				
         6
	</td><td class="cL">				
         7
	</td><td class="cL">				
         8
	</td><td class="cL">				
         9
	</td><td class="cLR">				
         10
	</td>
	</tr><tr class="t">				
        <td class="rL">				
        
  Fast Oil
	</td><td class="cL">				
         38.4
	</td><td class="cL">				
         39.6
	</td><td class="cL">				
         37.6
	</td><td class="cL">				
         40.2
	</td><td class="cL">				
         36.9
	</td><td class="cL">				
         39.4
	</td><td class="cL">				
         38.3
	</td><td class="cL">				
         39.6
	</td><td class="cL">				
         39.1
	</td><td class="cLR">				
         38.2
	</td>
	</tr><tr class="b">				
        <td class="rL">				
        
  Slicker
	</td><td class="cL">				
         39.8
	</td><td class="cL">				
         40.3
	</td><td class="cL">				
         39.7
	</td><td class="cL">				
         41.2
	</td><td class="cL">				
         38.6
	</td><td class="cL">				
         40.6
	</td><td class="cL">				
         39.9
	</td><td class="cL">				
         41.1
	</td><td class="cL">				
         40.8
	</td><td class="cLR">				
         39.8
	</td>
	</tr>
	</table>
    </div>
Find a $90\%$ confidence interval for the population mean difference.
</div>																

																Approximate confidence intervals using the Central Limit Theorem								
The possibility of generating approximate confidence intervals using the Central Limit Theorem is demonstrated by an example.
								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;34&nbsp;		</div>
Suppose that $X$ has a Poisson distribution with mean $\lambda$. A random sample of size 100 is taken from the distribution of $X$. Given that the sample mean is 6.1, obtain an approximate $90\%$ confidence interval for $\lambda$. Find also an approximate confidence interval for $\exp(-\lambda),$ the probability that $X$ takes the value zero.
</div>																

																Confidence intervals for variances								
Recall that
								<div class="quote">
       Let 								$X_{1},X_{2},\ldots,X_{n}$								 be a random sample from a normal distribution with mean 								$\mu$								 and variance 								$\sigma^2$								. Then 								$\displaystyle\frac{(n-1)S^{2}}{\sigma^{2}}$								 has a chi-squared distribution with 								$(n-1)$								 degrees of freedom.
    </div>								

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;35&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{n}$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. Derive a $100(1-\alpha)\%$ confidence interval for $\sigma^{2}.$</div>								

																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;36&nbsp;		</div>
Suppose a random sample of 25 observations was taken from a normal distribution, and it was found that the sample variance was equal to 10. Find a $95\%$ confidence interval for $\sigma^2.$</div>								

																

																Confidence intervals for ratio of variances								
Recall the following:
								<div class="quote">
       Let 								$X_{1},X_{2},\ldots,X_{m}$								 and 								$Y_{1},Y_{2},\ldots,Y_{n}$								 be  random samples from 								$X\sim N[\mu_{X},\sigma_{X}^{2}]$								 and 								$Y\sim N[\mu_{Y},\sigma_{Y}^{2}] $								 respectively. Let 								$S_{X}^{2}$								 and 								$S_{Y}^{2}$								 be the corresponding sample variances. It follows that 								$\displaystyle\frac{(m-1)S_{X}^{2}}{\sigma_{X}^{2}}\sim\chi^{2}(m-1)$								 and 								$\displaystyle\frac{(n-1)S_{Y}^{2}}{\sigma_{Y}^{2}}\sim\chi^{2}(n-1).$								 Hence 								$\displaystyle\frac{S_{X}^{2}/\sigma_{X}^{2}}{S_{Y}^{2}/\sigma_{Y}^{2}}\sim F(m-1,n-1).$
    </div>								

 A 								$100(1-\alpha)\%$								 confidence interval for 								$\displaystyle\frac{\sigma_{X}^{2}}{\sigma_{Y}^{2}}$								 is given by
								<div class="displaymath">
       \[\left[\frac{s_{X}^{2}}{s_{Y}^{2}}\frac{1}{F_{1-\alpha/2}(m-1,n-1)},\frac{s_{X}^{2}}{s_{Y}^{2}}\frac{1}{F_{\alpha/2}(m-1,n-1)}\right]
\]
    </div>								
This will be derived in an exercise sheet. Here, 								$F_{1-\alpha/2}(m-1,n-1)$								 is the 								$100(1-~\alpha/2)th$								 percentile of the F-distribution with 								$m-1$								 and 								$n-1$								 degrees of freedom.

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;37&nbsp;		</div>
Using the notation introduced in this section, obtain a $95\%$ confidence interval for $\sigma_{X}^{2}/\sigma_{Y}^{2}$ if $m=10$, $n=5$, $s_{X}^{2}=20$ and $s_{Y}^{2}=35.6.$</div>								

																

											
    </div>			
    </div>								<div class="section">
    													<div class="subsection">
    													
A statistical hypothesis is a statement concerning the distribution of a random variable e.g. the hypothesis that a coin is fair. This hypothesis concerns the distribution of the random variable generating the process, as tossing coins can be modelled as a binomial random variable, where we toss the coin 								$n$								 times and have a proportion 								$p$								 of heads. The hypothesis that a coin is fair is testing the hypothesis that 								$p=\frac{1}{2}$								 or not. To perform an hypothesis test we need to define lots of terminology, concepts and notation. As far as possible we will remain general and test hypotheses concerning an unknown population parameter 								$\theta.$								

The purpose of the test is to choose between two hypotheses. One is called the null hypothesis, and the other is called the alternative hypothesis.
The null hypothesis is denoted 								$H_0$								 and the alternative hypothesis is denoted 								$H_1$								.

The null hypothesis is commonly of the form:
																

Corresponding to this null hypothesis, the alternative hypothesis can be one of the following:
																

We thus have two options:
								<ol class="enumerate">
            					<li class="item">We reject 								$H_{0}$								, and accept 								$H_{1}$</li>								<li class="item">We accept 								$H_{0}$								, and reject 								$H_{1}$</li>			
        </ol>								

Consequently there are two types of errors that we can make:
								<ol class="enumerate">
            					<li class="item">The incorrect rejection of 								$H_{0}$								, known as a Type I error, which happens with probability 								$\alpha$								.</li>								<li class="item">The incorrect rejection of 								$H_{1}$								, known as a Type II error, which happens with probability 								$\beta$								.</li>			
        </ol>								

The probability of making a Type I error is denoted 								$\alpha$								, and this is known as the significance level of the hypothesis test. The probability of making a Type II error is denoted 								$\beta$								. Another concept used in hypothesis testing is the so-called power of the hypothesis test, and this is given by 								$1-\beta$								. Naturally for any hypothesis test, we desire 								$\alpha$								 to be as small as possible, and the power 								$1-\beta$								 to be as large as possible.

We now address the problem of how to create the decision rule of accepting 								$H_0$								 or rejecting 								$H_0$								. This is decided on the basis of a test statistic. We will now consider an example to illuminate some of these concepts described so far, and also outline each of the main steps necessary.

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;38&nbsp;		</div>
A beer-dispensing machine is supposed to deliver 20 fluid ounces of beer. The amount dispensed by the machine is thought to be normally distributed. 10 samples are measured from the machine, with the following results: 19.89, 19.90, 19.87, 19.94, 19.92, 19.90, 19.93, 19.91. The sample mean is given by 19.904, and the sample standard deviation is given by 0.0217. Test the hypothesis that the mean amount dispensed by the machine is indeed 20 fluid ounces.
</div>								

We will now deconstruct the steps necessary to perform this hypothesis test.

																

																

																

																

Interpretations of a
p-value should be along the following
lines:
								<ul class="itemize">
            					<li class="item">$p < 0.01$								; there is very strong evidence
for rejecting 								$H_{0}$								.</li>								<li class="item">$0.01 \leq p \leq 0.05$								; there is strong evidence
for rejecting 								$H_0$								.</li>								<li class="item">$p > 0.05$								; there is insufficient evidence
for rejecting 								$H_0$								.</li>			
        </ul>			
    </div>								<div class="subsection">
    													<div class="example">
		<div class="blockheading"> 
    Example&nbsp;39&nbsp;		</div>
It is claimed that a shop makes a profit of $\pounds850$, per week, on average. To test this, the manager records the weekly profit across five randomly selected weeks, and finds the average to be $\pounds905$, with standard deviation $\pounds50$. The manager asks the question ''Have profits increased significantly?". Perform a suitable hypothesis test at a $5\%$ significance level. Also approximate the p-value.
</div>								

																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;40&nbsp;		</div>
Ten students failed a diagnostic test, and are asked to resit after a period of intensive revision and support. The results are given below:
<div class="center">
       <table class="tabular">				
            <tr class="t">				
        <td class="cL">				
        
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        :  or col1-col2col3-col4 ...

  Student
	</td><td class="cL">				
         Test 1
	</td><td class="cLR">				
         Test 2
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        
  1
	</td><td class="cL">				
         30
	</td><td class="cLR">				
         40
	</td>
	</tr><tr>				
        <td class="cL">				
        
  2
	</td><td class="cL">				
         32
	</td><td class="cLR">				
         34
	</td>
	</tr><tr>				
        <td class="cL">				
        
  3
	</td><td class="cL">				
         27
	</td><td class="cLR">				
         50
	</td>
	</tr><tr>				
        <td class="cL">				
        
  4
	</td><td class="cL">				
         30
	</td><td class="cLR">				
         38
	</td>
	</tr><tr>				
        <td class="cL">				
        
  5
	</td><td class="cL">				
         35
	</td><td class="cLR">				
         33
	</td>
	</tr><tr>				
        <td class="cL">				
        
  6
	</td><td class="cL">				
         32
	</td><td class="cLR">				
         45
	</td>
	</tr><tr>				
        <td class="cL">				
        
  7
	</td><td class="cL">				
         25
	</td><td class="cLR">				
         40
	</td>
	</tr><tr>				
        <td class="cL">				
        
  8
	</td><td class="cL">				
         20
	</td><td class="cLR">				
         30
	</td>
	</tr><tr>				
        <td class="cL">				
        
  9
	</td><td class="cL">				
         31
	</td><td class="cLR">				
         38
	</td>
	</tr><tr class="b">				
        <td class="cL">				
        
  10
	</td><td class="cL">				
         30
	</td><td class="cLR">				
         42
	</td>
	</tr>
	</table>
    </div>
Conduct an appropriate hypothesis test to determine if the intensive revision and support was effective.
</div>								

																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;41&nbsp;		</div>
A petrol additive is supposed to increase the mpg (miles per gallon) of cars. A company runs a fleet of 30 identical cars. For a period of two months, 20 of the cars were run on petrol with the additive, whilst the other 10 were run without. Cars without the additive gave an average mpg of 38.2 with standard deviation 5.3. Cars with the additive gave an average mpg of 45.6, with standard deviation 4.7. Clearly stating your assumptions, perform a suitable hypothesis test to compare the average mpg between cars that received the additive, and those that didn't.
</div>								

																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;42&nbsp;		</div>
Two samples of size 16 and 26 taken from two normally distributed populations had sample variances of 29.6 and 9.8 respectively. Use an appropriate hypothesis test to test the assertion that the population variances are equal.
</div>								

																


			
    </div>								<div class="subsection">
    													

The hypothesis tests considered so far are based on scenarios when the data are normally distributed. This small section of notes will show that the central limit theorem can be used to design similar tests when the sample sizes are large enough for this theorem to apply.

Suppose therefore that 								$X$								 is a random variable with mean 								$\mu$								 and variance 								$\sigma^{2}$								. Let 								$X_{1},\ldots,X_{n}$								 be a random sample from the distribution of 								$X$								, then by the central limit theorem the statistic 								<div class="displaymath">
       \[Z=\frac{\bar{X}-\mu_{0}}{\sigma/\sqrt{n}}\]
    </div>								 under a null hypothesis 								$H_{0}:\mu=\mu_{0}$								 is approximately 								$N[0,1]$								 for large 								$n$								.

This method is widely applicable, and the example that will be demonstrated here involves the binomial distribution. The binomial distribution is characterised by two parameters: 								$p$								 and 								$n$								. 								$n$								 is the number of trials, and 								$p$								 is the probability of success. Lets imagine that we want to test hypotheses concerning 								$p$								. The theory is contained here.

Suppose that we observe a sequence of Bernoulli trials with unknown success probability 								$p$								, and we wish to test the null hypothesis against a suitable one or two tailed alternative. Let 								$Y$								 be the number of successes obtained in 								$n$								 trials and let 								$\hat{p}=\frac{Y}{n}$								 be the observed proportion of successful trials.

It follows that 								$Y$								 has a binomial distribution, 								$B[n,p]$								, which is approximately 								$N[np,np(1-p)]$								 for large 								$n$								. Thus 								$\hat{p}$								 is approximately 								$N[p,\frac{p(1-p)}{n}]$								 (why?) and the standardised statistic 								<div class="displaymath">
       \[z=\frac{\hat{p}-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}\]
    </div>								 is approximately 								$N[0,1]$								 under a null hypothesis 								$H_{0}:p=p_0$								. We can then reject or accept 								$H_{0}$								 by comparing 								$z$								 with the appropriate critical value of the normal distribution.

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;43&nbsp;		</div>
A drug company claims in an advertisement that $60\%$ of people suffering from a certain complaint gain instant relief by using a particular product. In a random sample, 106 out of 200 did gain instant relief. Test the validity of this claim, and find the p-value.
</div>								

																

			
    </div>								<div class="subsection">
    													
The formal definition of a 								$100(1-\alpha)\%$								 confidence interval is that it is the set of values of a parameter that would be accepted as the null hypothesis in a hypothesis test with significance level 								$\alpha.$								

So, a 								$95\%$								 confidence interval corresponds to a test with significance level 								$0.05.$								 However the two should not be interchanged. The role of a confidence interval is to provide an interval estimator for a parameter with an associated level of uncertainty. A hypothesis test is used to gauge the strength of the evidence in the data for or against a specified null hypothesis, and this evidence is most clearly stated by quoting the p-value of the test.

																


			
    </div>								<div class="subsection">
    													
The principles of statistical hypothesis testing can be summarized in five steps.

1. Set up a null hypothesis about a parameter for testing. Also decide the level of evidence that is needed to reject this null hypothesis (see steps 4 and 5).

2. Take a random sample from the population.

3. Estimate the parameter from the sample and use this to calculate the value of a test statistic, whose distribution is known when the null hypothesis is known.

Steps 4 and 5 depend on whether the significance level approach is adopted, or the p-value approach. The latter is preferred, if it is possible to calculate or approximate the p-value, because it gives more information about the strength of evidence for or against the null hypothesis that is contained in the observations.

4a. In the 								$p$								-value approach, calculate the probability, using the  probability distribution of the test statistic, of a result as extreme or more so than the one observed.

5a. If the probability calculated in Step 4a is judged to be small, reject the null hypothesis, otherwise accept the null hypothesis.

4b. In the significance level approach a pre-determined significance level, a smallish probability (typically 0.05 or 0.01) is predetermined in Step 1 as the level of evidence needed to reject the null hypothesis. Using this significance level a critical value of the test statistic is calculated that determines a critical region and an acceptance region.

5b. If the value of the test statistic is in the critical region, the null hypothesis is rejected (at the pre-determined level of significance). If the value of the test statistic is in the acceptance region, accept the null hypothesis.

			
    </div>								<div class="subsection">
    													
In previous sections we have designed tests, using only the significance level 								$\alpha$								 and the probability distribution of the test statistic when 								$H_{0}$								 is true. We cannot, in general, compute the Type II error probability (								$\beta$								) as it varies with the parameter values consistent with 								$H_{1}$								.

We can however regard it as a function of these values and knowledge of this function helps us to assess the performance of the test. We make the following definition:

								<span class="textbf">
       Definition:
    </span>								 Consider a test of two hypotheses, 								$H_{0}$								 and 								$H_{1}$								, concerning a parameter 								$\theta.$								 The power function of the test, denoted by 								$\pi(\theta)$								 is the probability of rejecting 								$H_{0}$								, and hence accepting 								$H_{1},$								 expressed as a function of 								$\theta.$								 If 								$C$								 is the critical region, we use the notation 								$P(C;\theta)$								 to denote this, i.e. 								<div class="displaymath">
       \[\pi(\theta)=P(C;\theta).\]
    </div>								

The following example shows how the power function can be calculated.

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;44&nbsp;		</div>
A random variable $X$ has the distribution $N[\theta,1].$ Using a random sample of size 100 and a $5\%$ significance level, compute the power function for the following hypothesis test:
<div class="displaymath">
       \[
H_{0}: \theta=0; \qquad H_{1}: \theta\neq 0\,.
\]
    </div></div>								

																

The graph of the power function for the above example is given below.

																																

Three points are worth noting:
								<ol class="enumerate">
            					<li class="item">$\pi(\theta)$								 is a probability and lies between 0 and 1.</li>								<li class="item">$\pi(0)=P(C;0)=\alpha=0.05.$</li>								<li class="item">As 								$|\theta|$								 increases, 								$\pi(\theta)$								 approaches unity. At the same time the type II error probability 								$\beta$								 approaches zero since 								$\beta=1-\pi(\theta).$</li>			
        </ol>								

A good test will reject 								$H_{0}$								 with low probability if it is true but will reject it with high probability if it is false. Thus 								$\pi(\theta)$								 should be close to zero for values of 								$\theta$								 consistent with 								$H_{0}$								 and close to unity otherwise. In the above example the ideal power function is thus:
								<div class="displaymath">
       \[\pi(\theta)=\left\{\begin{array}{ll}
      0, & \hbox{if}\;\;\theta=0 \\
      1, & \hbox{if}\;\;\theta\neq 0.
    \end{array}\right.
\]
    </div>								
The ideal power function is not attainable in practice but the closer the actual power function is to this, the better the test. We can usually improve the power function by increasing the sample size.

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;45&nbsp;		</div>
Let $X_{1},X_{2},\ldots,X_{n}$ be a random sample from the distribution $N[\mu,1].$ Construct a test of the hypotheses <div class="displaymath">
       \[
H_{0}: \mu=0; \qquad H_{1}: \mu\neq 0\,.
\]
    </div>
at the $5\%$ significance level and plot its power function for sample sizes of 25, 100 and 500.
</div>								

																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;46&nbsp;		</div>
Let $X$ have a uniform distribution on $[0,\theta]$. Using a random sample of size 12, we wish to test the hypotheses
<div class="displaymath">
       \[
H_{0}: \theta=6; \qquad H_{1}: \theta>6\,.
\]
    </div>
at the $10\%$ level of significance. Which of the statistics, the sample mean $\bar{X}$ and the sample maximum $Z,$ gives the better test?
</div>																




			
    </div>								<div class="subsection">
    																					

																Introduction								

The vast majority of the hypothesis tests investigated so far assume that the data is sampled from a normally distributed random variable. When the random variable is specified as something else, such as the Poisson distribution, or even if it is unknown, we can use the Central Limit Theorem to construct an approximate hypothesis test.

We will now study hypothesis tests designed to investigate experiments where the outcome can fall into one of a finite number of categories.

																The Chi-square test								
Suppose that the result of an experiment is classified into one of 								$k$								 categories 								$c_{1},c_{2},\ldots,c_{k}$								, and that for a given model the expected numbers of outcomes in each category are 								$E_{1},E_{2},\ldots,E_{k}$								 respectively. If 								$O_{1},O_{2},\ldots,O_{k}$								 are the observed numbers in each category, the chi-square statistic is given by
								<div class="displaymath">
       \[\chi^{2}=\sum_{i=1}^{k}\frac{(O_{i}-E_{i})^2}{E_{i}}\]
    </div>								
The statistic 								$\chi^2$								 has a distribution which is approximately chi-square with 								$k-m-1$								 degrees of freedom, where 								$m$								 is the number of parameters that need to be estimated from the data to fit any theoretical distribution. The approximation is good, provided 								$E_{i}>5$								 for all 								$i$								. If any 								$E_{i}<5,$								 the outcomes may be grouped to ensure that all 								$E_{i}\geq 5.$								

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;47&nbsp;		</div>
A die is rolled 60 times, and the outcome of this experiment is given below. Test at both a $5\%$ and $1\%$ significance level the null hypothesis $H_{0}$ that the die is fair
<div class="center">
       <table class="tabular">				
            <tr class="t">				
        <td class="lL">				
        
	</td>
	</tr><tr class="t">				
        <td class="lL">				
        :  or col1-col2col3-col4 ...

  Face
	</td><td class="cL">				
         1
	</td><td class="cL">				
         2
	</td><td class="cL">				
         3
	</td><td class="cL">				
         4
	</td><td class="cL">				
         5
	</td><td class="cLR">				
         6
	</td>
	</tr><tr class="tb">				
        <td class="lL">				
        
  Outcome $O_{i}$
	</td><td class="cL">				
         15
	</td><td class="cL">				
         7
	</td><td class="cL">				
         4
	</td><td class="cL">				
         11
	</td><td class="cL">				
         6
	</td><td class="cLR">				
         17
	</td>
	</tr>
	</table>
    </div></div>																

It is possible to write 								$\chi^2$								 in an easier computational form. Prove the following theorem yourself, in the space provided.
								<div class="theorem">
		<div class="blockheading"> 
    Theorem&nbsp;48&nbsp;		</div>
Let $N$ be the total number of observations ( in the previous example $N=60$). Then
<div class="displaymath">
       \[\chi^{2}=\sum_{i=1}^{k}\frac{(O_{i}-E_{i})^2}{E_{i}}=\sum_{i=1}^{k}\frac{O_{i}^{2}}{E_{i}}-N \, .
\]
    </div></div>								

																

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;49&nbsp;		</div>
100 squares each 1 meter square were randomly placed in a field where daffodils were growing. The number of clumps of daffodils in each square was counted and the following table of these observed numbers was drawn up.
<div class="center">
       <table class="tabular">				
            <tr class="t">				
        <td class="cL">				
        
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        :  or col1-col2col3-col4 ...


	</td><td class="cL">				
         0
	</td><td class="cL">				
         1
	</td><td class="cL">				
         2
	</td><td class="cL">				
         3
	</td><td class="cL">				
         4
	</td><td class="cL">				
         5
	</td><td class="cLR">				
        $\geq$ 6
	</td>
	</tr><tr class="tb">				
        <td class="cL">				
        $O_{i}$
	</td><td class="cL">				
         4
	</td><td class="cL">				
         17
	</td><td class="cL">				
         20
	</td><td class="cL">				
         22
	</td><td class="cL">				
         15
	</td><td class="cL">				
         15
	</td><td class="cLR">				
         7
	</td>
	</tr>
	</table>
    </div>
A Poisson distribution is suggested as a model for the number of clumps of daffodils in a square. Estimate an appropriate value for the mean of this distribution using the data in the table above. Calculate a table of expected numbers to fit the Poisson distribution to these data and perform a chi-square goodness of fit test to determine if the model is a good fit to the observed data.
</div>								

																

																Contingency tables								
In the previous example, the probabilities used in the null hypothesis were obtained from theoretical considerations.  We often want to test the hypothesis that effects are independent without having a theory to predict the relevant probabilities.  In this case, the probabilities must be estimated from the contingency table.

Contingency means dependence, so a contingency table is simply a table that displays how two or more characteristics depend on each other.  For example, a contingency table, where Effect 1 has three categories 								$I, II, III$								 and Effect 2 has four categories 								$A, B, C, D$								 is:
								<div class="center">
       <table class="tabular">				
            					<tr class="t">				
        					<td class="lL">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					:  or 																col1-col2																col3-col4								 ...

			
	</td>								<td class="cL">				
        					$A$			
	</td>								<td class="cL">				
        					$B$			
	</td>								<td class="cL">				
        					$C$			
	</td>								<td class="cL">				
        					$D$			
	</td>								<td class="cLR">				
        
	</td>			
	</tr>								<tr class="t">				
        					<td class="lL">				
        					$I$			
	</td>								<td class="cL">				
        					$O_{11}$			
	</td>								<td class="cL">				
        					$O_{12}$			
	</td>								<td class="cL">				
        					$O_{13}$			
	</td>								<td class="cL">				
        					$O_{14}$			
	</td>								<td class="cLR">				
        					$R_{1}$			
	</td>			
	</tr>								<tr>				
        					<td class="lL">				
        					$II$			
	</td>								<td class="cL">				
        					$O_{21}$			
	</td>								<td class="cL">				
        					$O_{22}$			
	</td>								<td class="cL">				
        					$O_{23}$			
	</td>								<td class="cL">				
        					$O_{24}$			
	</td>								<td class="cLR">				
        					$R_{2}$			
	</td>			
	</tr>								<tr>				
        					<td class="lL">				
        					$III$			
	</td>								<td class="cL">				
        					$O_{31}$			
	</td>								<td class="cL">				
        					$O_{32}$			
	</td>								<td class="cL">				
        					$O_{33}$			
	</td>								<td class="cL">				
        					$O_{34}$			
	</td>								<td class="cLR">				
        					$R_{3}$			
	</td>			
	</tr>								<tr class="tb">				
        					<td class="lL">				
        
	</td>								<td class="cL">				
        					$C_{1}$			
	</td>								<td class="cL">				
        					$C_{2}$			
	</td>								<td class="cL">				
        					$C_{3}$			
	</td>								<td class="cL">				
        					$C_{4}$			
	</td>								<td class="cLR">				
        					$N$			
	</td>			
	</tr>			
	</table>
    </div>								
where the 								$O_{ij}$								 is the number of observations observed in the 								$i$								th row and 								$j$								th column, 								$R_{i}$								 are row totals, 								$C_{j}$								 are column totals, and 								$N$								 is the total number of observations.

Under the null hypothesis that the effects are independent, the probability associated with cell 								$(i,j)$								 is estimated by:
								<div class="displaymath">
       \[
P_{ij}=\frac{R_{i}}{N}\times\frac{C_{j}}{N}=\frac{R_{i}C_{j}}{N^{2}}\]
    </div>								
and hence the expected number for each cell 								$(i,j)$								 is given by:
								<div class="displaymath">
       \[
E_{ij}=N \times P_{ij}=\frac{R_{i}C_{j}}{N}\,.
\]
    </div>								

The statistic
								<div class="displaymath">
       \[\chi^{2}=\sum_{i=1}^{r}\sum_{j=1}^{c}\frac{(O_{ij}-E_{ij})^{2}}{E_{ij}}=\sum_{i=1}^{r}\sum_{j=1}^{c}\frac{O_{ij}^{2}}{E_{ij}}-N
\]
    </div>								
where 								$r$								 is the number of rows, and 								$c$								 is the number of columns follows a chi-square distribution with 								$(r-1)(c-1)$								 degrees of freedom.

								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;50&nbsp;		</div>
The distribution of five plant species $A, B, C, D, E$ is being investigated at three different locations $I, II, III$. The contingency table of the results is presented below.
<div class="center">
       <table class="tabular">				
            <tr class="t">				
        <td class="cL">				
        
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        :  or col1-col2col3-col4 ...


	</td><td class="cL">				
        $A$
	</td><td class="cL">				
        $B$
	</td><td class="cL">				
        $C$
	</td><td class="cL">				
        $D$
	</td><td class="cLR">				
        $E$
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        $I$
	</td><td class="cL">				
         10
	</td><td class="cL">				
         22
	</td><td class="cL">				
         38
	</td><td class="cL">				
         8
	</td><td class="cLR">				
         66
	</td>
	</tr><tr>				
        <td class="cL">				
        $II$
	</td><td class="cL">				
         27
	</td><td class="cL">				
         62
	</td><td class="cL">				
         120
	</td><td class="cL">				
         30
	</td><td class="cLR">				
         200
	</td>
	</tr><tr class="b">				
        <td class="cL">				
        $III$
	</td><td class="cL">				
         45
	</td><td class="cL">				
         100
	</td><td class="cL">				
         207
	</td><td class="cL">				
         49
	</td><td class="cLR">				
         342
	</td>
	</tr>
	</table>
    </div>
Use an appropriate statistical test to address the problem of whether the species of plant is independent of the location.
</div>								

																

This is the end of the self-study section. To complete your study and to evaluate your understanding of this material, look through some text books to find your own examples. Attempt them, and write up your solutions.

																

			
    </div>								<div class="subsection">
    																					Introduction and motivation								
In this section the analysis is described of an experiment that has several groups of observations. These could either be different levels of the same factor, such as concentrations of a chemical, or several different factors that might affect the observations made. The main objective is to determine if there are significant differences amongst the means of the levels, but the analysis is based on an examination of variation, and is often called the one-way analysis of variance. The one-way factorial experiment can be viewed as an extension of  hypothesis tests for two means, to an experiment where there are more than two groups to be compared. The ideas presented in this section can be extended to factorial experiments, where several factors are measured simultaneously at all combinations of levels. These multi-factorial experiments will not be described in this module.

A valid question is ''why do we need to construct another hypothesis test to compare the difference of three or more means, when we can do lots of tests comparing pairs of means?". Remember that every single statistical test that you perform runs the risk of making a Type I error. If you do lots of hypothesis tests on the same data, then you accumulate more and more risk of making a Type I error. Hence, if we can do one statistical test that answers all our hypothesis simultaneously, then this is often preferable.

																Notation and set-up								
Suppose there are 								$m$								 groups of data. The notation for the 								$j$								th observation in the 								$i$								th group is 								$x_{ij}$								.

There can be different numbers of observations from group to group, so the number of observations in the 								$i$								th group is 								$n_i$								.

So 								$j$								 runs from 1 to 								$n_i$								, in the 								$i$								th group, and 								$i$								 runs from 1 to 								$m$								.

Thus the data might be arranged as:
								<div class="center">
       <table class="tabular">				
            					<tr>				
        					<td class="c">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="c">				
        					:  or 																col1-col2																col3-col4								 ...

  Group 1:			
	</td>								<td class="c">				
        					$x_{11}$			
	</td>								<td class="c">				
        					$x_{12}$			
	</td>								<td class="c">				
        					$\ldots$			
	</td>								<td class="c">				
        					$x_{1n_{1}}$			
	</td>			
	</tr>								<tr>				
        					<td class="c">				
        					
  Group 2:			
	</td>								<td class="c">				
        					$x_{21}$			
	</td>								<td class="c">				
        					$x_{22}$			
	</td>								<td class="c">				
        					$\ldots$			
	</td>								<td class="c">				
        					$x_{2n_{2}}$			
	</td>			
	</tr>								<tr>				
        					<td class="c">				
        					$\vdots$			
	</td>								<td class="c">				
        					$\vdots$			
	</td>								<td class="c">				
        					$\vdots$			
	</td>								<td class="c">				
        					$\vdots$			
	</td>								<td class="c">				
        					$\vdots$			
	</td>			
	</tr>								<tr>				
        					<td class="c">				
        					
  Group 								$m$								:			
	</td>								<td class="c">				
        					$x_{m1}$			
	</td>								<td class="c">				
        					$x_{m2}$			
	</td>								<td class="c">				
        					$\ldots$			
	</td>								<td class="c">				
        					$x_{mn_{m}}$			
	</td>			
	</tr>			
	</table>
    </div>								
The null hypothesis here is 								$H_{0}: \mu_{1}=\mu_{2}=\ldots=\mu_{m}$								, where 								$\mu_{i}$								 is the population mean corresponding to the 								$i$								th group. The alternative hypothesis is 								$H_{1}:\mu_{a}\neq\mu_{b}$								, where 								$a\neq b$								.

																

																Possible sources of variation								

																

We thus have three sums of squares:
								<ol class="enumerate">
            					<li class="item">Total sum of squares</li>								<li class="item">Between group sum of squares</li>								<li class="item">Within group sum of squares (also known as error, or residual sum of squares)</li>			
        </ol>								
We now describe each in turn.

																

																

																

																ANOVA table								
The comparison of sums of squares is done in a table called the analysis of variance table, because the variation in the observations is being partitioned into part that is explained by the data because they are sampled from different groups (the between groups component), and a part that remains unexplained (the residual component).

								<div class="center">
       <table class="tabular">				
            					<tr class="t">				
        					<td class="cL">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="cL">				
        					:  or 																col1-col2																col3-col4								 ...

  Source of variation			
	</td>								<td class="cL">				
        					 df			
	</td>								<td class="cL">				
        					 ss			
	</td>								<td class="cL">				
        					 ms			
	</td>								<td class="cLR">				
        					 F-ratio			
	</td>			
	</tr>								<tr class="t">				
        					<td class="cL">				
        					[20pt]
  Between groups			
	</td>								<td class="cL">				
        					$m-1$			
	</td>								<td class="cL">				
        					$\displaystyle\sum_{i=1}^{m}\frac{X_{i\cdot}^{2}}{n_{i}}-\frac{X_{\cdot\cdot}^{2}}{N}$			
	</td>								<td class="cL">				
        					$s_{G}^{2}$			
	</td>								<td class="cLR">				
        					$\displaystyle\frac{s_{G}^{2}}{s^{2}}$			
	</td>			
	</tr>								<tr class="t">				
        					<td class="cL">				
        					[20pt]
  Within groups (error or residual)			
	</td>								<td class="cL">				
        					$N-m$			
	</td>								<td class="cL">				
        					$\displaystyle\sum_{i=1}^{m}\sum_{j=1}^{n_{i}}x_{ij}^{2}-\sum_{i=1}^{m}\frac{X_{i\cdot}^{2}}{n_{i}}$			
	</td>								<td class="cL">				
        					$s^{2}$			
	</td>								<td class="cLR">				
        					 -			
	</td>			
	</tr>								<tr class="t">				
        					<td class="cL">				
        					[20pt]
  Total			
	</td>								<td class="cL">				
        					$N-1$			
	</td>								<td class="cL">				
        					$\displaystyle\sum_{i=1}^{m}\sum_{j=1}^{n_{i}}x_{ij}^{2}-\frac{X_{\cdot\cdot}^2}{N}$			
	</td>								<td class="cL">				
        					 -			
	</td>								<td class="cLR">				
        					 -			
	</td>			
	</tr>								<tr class="t">				
        					<td class="cL">				
        					[20pt]			
	</td>			
	</tr>			
	</table>
    </div>								


The mean squares (ms) are calculated by dividing the sum of squares (ss) by degrees of freedom (df). The F-ratio is the ratio of between groups mean square to error mean square.

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;51&nbsp;		</div>
Under the null hypothesis $H_{0}: \mu_{1}=\mu_{2}=\ldots=\mu_{m}$, then the F-ratio $\displaystyle\frac{s_{G}^{2}}{s^{2}}$ follows an F-distribution with $(m - 1)$ and $(N - m)$ degrees of freedom.
</div>								

The percentiles of the F distribution with 								$(m - 1)$								 and 								$(N - m)$								 degrees of freedom are used to judge the significance of the observed F-ratio. If the observed F-ratio exceeds the tabulated 95th percentile, for example, then with a 								$5\%$								 level of significance the null hypothesis (of equal group means) is rejected.

																Multiple comparison tests								
Although one-way ANOVA can be used to identify that there are statistically significant differences in the means of m groups of data, it does not identify which particular groups differ in this respect from the others.

Various tests have been suggested to do this. The simplest to describe is Fisher's least significant difference (LSD) test.

																


																Example								<div class="example">
		<div class="blockheading"> 
    Example&nbsp;52&nbsp;		</div>
The systolic blood pressure was measured for 7 human subjects, with 5 replicate observations being made for each subject. The results obtained (in mmHg) are tabulated below, together with some totals.
<div class="center">
       <table class="tabular">				
            <tr class="t">				
        <td class="cL">				
        
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        :  or col1-col2col3-col4 ...

                                  Subject
	</td><td class="cL">				
         Systolic blood pressure
	</td><td class="cLR">				
         Totals
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        
                                  1
	</td><td class="cL">				
         108 104 108 120 108
	</td><td class="cLR">				
         548
	</td>
	</tr><tr>				
        <td class="cL">				
        
                                  2
	</td><td class="cL">				
         118 104 118 120 128
	</td><td class="cLR">				
         588
	</td>
	</tr><tr>				
        <td class="cL">				
        
                                  3
	</td><td class="cL">				
         124 118 120 122 124
	</td><td class="cLR">				
         608
	</td>
	</tr><tr>				
        <td class="cL">				
        
                                  4
	</td><td class="cL">				
         126 122 120 120 132
	</td><td class="cLR">				
         620
	</td>
	</tr><tr>				
        <td class="cL">				
        
                                  5
	</td><td class="cL">				
         128 124 118 112 142
	</td><td class="cLR">				
         624
	</td>
	</tr><tr>				
        <td class="cL">				
        
                                  6
	</td><td class="cL">				
         130 128 138 116 124
	</td><td class="cLR">				
         636
	</td>
	</tr><tr>				
        <td class="cL">				
        
                                  7
	</td><td class="cL">				
         152 128 132 150 142
	</td><td class="cLR">				
         704
	</td>
	</tr><tr class="b">				
        <td class="cL">				
        
	</td><td class="cL">				
        
	</td><td class="cLR">				
         4328
	</td>
	</tr>
	</table>
    </div>
Complete an analysis of variance table for these data and test to determine if there are significant differences amongst the means of the subjects. Follow up your analysis by attempting to identify which groups differ from which.
</div>								

																

																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																


											
    </div>			
    </div>								<div class="section">
    													<div class="subsection">
    													
We now consider data consisting of pairs of observations 								$(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})$								 where the 								$x$								 and 								$y$								 values may be related. We use the data to investigate the nature of the relationship between the two variables.

Specifically we will consider that the observed data 								$(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})$								  follows a linear relationship of the form 								<div class="displaymath">
       \[Y=\alpha+\beta x +\varepsilon\]
    </div>								
where 								$\alpha$								 and 								$\beta$								 are constants and 								$\varepsilon$								 is a random error term with zero mean. Note that the errors are assumed to be independent. This is equivalent to assuming that 								$E[Y]=\alpha+\beta x$								 so that the mean (or expected value) of 								$Y$								 is a linear function of 								$x$								.

In general the variable 								$x$								 is called the independent variable and the variable 								$Y$								 is called the dependent variable. Examples include the following:
								<div class="center">
       <table class="tabular">				
            					<tr class="t">				
        					<td class="cL">				
        								
	</td>			
	</tr>								<tr class="t">				
        					<td class="cL">				
        					:  or 																col1-col2																col3-col4								 ...
								$x$			
	</td>								<td class="cLR">				
        					$Y$			
	</td>			
	</tr>								<tr class="t">				
        					<td class="cL">				
        					
  Amount spent advertising a product			
	</td>								<td class="cLR">				
        					 Sales of the product			
	</td>			
	</tr>								<tr>				
        					<td class="cL">				
        					
  Age			
	</td>								<td class="cLR">				
        					 Height			
	</td>			
	</tr>								<tr>				
        					<td class="cL">				
        					
  Distance			
	</td>								<td class="cLR">				
        					 Time to run distance			
	</td>			
	</tr>								<tr class="b">				
        					<td class="cL">				
        					
  Weight of fertilizer			
	</td>								<td class="cLR">				
        					 Tomato yield			
	</td>			
	</tr>			
	</table>
    </div>								

The primary aim of regression is to estimate the parameters 								$\alpha$								 and 								$\beta$								. We consider this in the next section.

			
    </div>								<div class="subsection">
    													
Suppose we have 								$n$								 pairs of observations 								$(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})$								  from the model 								<div class="displaymath">
       \[Y=\alpha+\beta x +\varepsilon\]
    </div>								
where 								$\alpha$								 (the intercept) and 								$\beta$								 (the slope) are constants and 								$\varepsilon$								 is a random error term with zero mean.



																

The principle of least squares regression, in this scenario is to find 								$\alpha$								 and 								$\beta$								 such that 								<div class="displaymath">
       \[\sum_{i=1}^{n}(y_{i}-\alpha-\beta x_{i})^{2}\]
    </div>								 is as small as possible. The values of 								$\alpha$								 and 								$\beta$								 which minimize this sum are known as least squares estimates.

								<div class="theorem", id="th_reg">
		<div class="blockheading"> 
    Theorem&nbsp;53&nbsp;		</div>
Suppose we have $n$ pairs of observations $(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{n},y_{n})$  from the model <div class="displaymath">
       \[Y=\alpha+\beta x +\varepsilon\]
    </div>
where $\alpha$ and $\beta$ are constants and $\varepsilon$ is a random error term with zero mean. The least squares estimators of $\alpha$ and $\beta$, denoted $\hat{\alpha}$ and $\hat{\beta}$ are given by:
<div class="eqnarray">
       \begin{eqnarray}% \nonumber % Remove numbering (before each equation)
\hat{\alpha} &=& \bar{y}-\hat{\beta}\bar{x}\\\hat{\beta} &=& \frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}\, .
\end{eqnarray}
    </div></div>								

																

If we write:
								<div class="eqnarray">
       \begin{eqnarray}
S_{xx}&=&\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}=\sum_{i=1}^{n}(x_{i}-\bar{x})x_{i}=\sum_{i=1}^{n}x_{i}^{2}-\frac{(\sum_{i=1}^{n}x_{i})^{2}}{n}=\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2}\\
S_{xy}&=&\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})=\sum_{i=1}^{n}(x_{i}-\bar{x})y_{i}=\sum_{i=1}^{n}x_{i}(y_{i}-\bar{y})=\sum_{i=1}^{n}x_{i}y_{i}-\frac{\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}y_{i}\right)}{n}\\&=&\sum_{i=1}^{n}x_{i}y_{i}-n\bar{x}\bar{y}\end{eqnarray}
    </div>								
where 								$S_{yy}$								 is defined analogously to 								$S_{xx}$								 with the obvious changes, then 								<div class="displaymath">
       \[\hat{\beta}=\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}=\frac{S_{xy}}{S_{xx}}=\frac{\sum_{i=1}^{n}x_{i}y_{i}-n\bar{x}\bar{y}}{\sum_{i=1}^{n}x_{i}^{2}-n\bar{x}^{2}}\,.\]
    </div>								

The minimum value of 								$\sum_{i=1}^{n}(y_{i}-\alpha-\beta x_{i})^{2}$								 is given by 								<div class="displaymath">
       \[\sum_{i=1}^{n}(y_{i}-\hat{\alpha}-\hat{\beta} x_{i})^{2}, \]
    </div>								 and is known as the residual sum of squares. We denote this by 								$SS_{error}$								.
								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;54&nbsp;		</div>
Let $\hat{\alpha}$ and $\hat{\beta}$ be defined as in Theorem <a href="ch00sec04.html#th_reg">53</a>. The residual sum of squares, $SS_{error}$ may be written
<div class="displaymath">
       \[SS_{error}=\sum_{i=1}^{n}(y_{i}-\hat{\alpha}-\hat{\beta} x_{i})^{2}=S_{yy}-\frac{S^{2}_{xy}}{S_{xx}}.\]
    </div></div>								
Try to prove this yourself. We will use this result later.

			
    </div>								<div class="subsection">
    													
Recall that we have 								$n$								 pairs of observations from the model 								$Y_{i}=\alpha+\beta x_{i} +\varepsilon_{i}$								.

Let 								$Y_{i}$								 denote the random variable whose observed value is 								$y_{i}$								. It is a random variable due to the addition of the random error term 								$\varepsilon_{i}$								. To consider properties of the estimator 								$\hat{\beta}$								 (which will enable us to consider properties of the estimator 								$\hat{\alpha}$								 as well as the predicted value of 								$y$								 given 								$x$								) we write
								<div class="displaymath">
       \[\hat{\beta}=\sum_{i=1}^{n}\frac{(x_{i}-\bar{x})(Y_{i}-\bar{Y})}{S_{xx}}.
\]
    </div>								

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;55&nbsp;		</div>
  We may write
  <div class="displaymath">
       \[\hat{\beta}=\sum_{i=1}^{n}\frac{(x_{i}-\bar{x})(Y_{i}-\bar{Y})}{S_{xx}}=\sum_{i=1}^{n}\frac{(x_{i}-\bar{x})Y_{i}}{S_{xx}}.
  \]
    </div></div>								
We use this form to derive properties of 								$\hat{\beta}$								 (and other parameters).

																

In order to construct confidence intervals and hypothesis tests for estimators of the parameters of the model, and for predictions of 								$y$								 using it, we need to make some distributional assumptions concerning the random error term 								$\varepsilon_{i}$								 of the model:
								<ol class="enumerate">
            					<li class="item">They are all independent.</li>								<li class="item">They have zero expectation.</li>								<li class="item">They all have the same variance 								$\sigma^{2}$								 (independent of 								$x$								 and 								$i$								).</li>								<li class="item">They are normally distributed.</li>			
        </ol>								

Straight away, we can derive the following facts using the above assumptions.
								<div class="eqnarray">
       \begin{eqnarray}% \nonumber % Remove numbering (before each equation)

  E[Y_{i}] &=& E[\alpha+\beta x_{i} +\varepsilon_{i}]=E[\alpha+\beta x_{i}] +E[\varepsilon_{i}]=\alpha+\beta x_{i}, \\
   Var[Y_{i}] &=& Var[\alpha+\beta x_{i} +\varepsilon_{i}]=Var[\alpha+\beta x_{i}] +Var[\varepsilon_{i}]=\sigma^{2}.
\end{eqnarray}
    </div>								


We will use these facts in what follows.
																								Mean and variance of 								$\hat{\beta}$								

																

																Mean and variance of 								$\hat{\alpha}$																

																Mean and variance of 								$\hat{y_{0}}$								
Suppose we are given a value of 								$x$								, say 								$x_{0}$								. We may be asked to estimate the mean response 								$y_{0}$								 at this given value. Since 								$y_{0}=\alpha+\beta x_{0}$								 the obvious estimator of 								$y_{0}$								 is 								$\hat{y_{0}}=\hat{\alpha}+\hat{\beta} x_{0}.$								 We can consider the mean and variance of this estimate.
																

An examination of the expressions for 								$\hat{\beta}$								, 								$\hat{\alpha}$								 and 								$\hat{y_{0}}$								 shows that they are all expressible as linear combinations of 								$Y_{1},Y_{2},\ldots,Y_{n}$								. It follows that if the 								$\varepsilon_{i}$								's are normally distributed then so are 								$\hat{\beta}$								, 								$\hat{\alpha}$								 and 								$\hat{y_{0}}$								. In this case we can summarise the preceding results by stating that
								<div class="eqnarray">
       \begin{eqnarray}% \nonumber % Remove numbering (before each equation)
\hat{\beta} &\sim& N\left[\beta,\frac{\sigma^{2}}{S_{xx}}\right], \\\hat{\alpha} &\sim& N\left[\alpha,\sigma^{2}\left(\frac{1}{n}+\frac{\bar{x}^{2}}{S_{xx}}\right)\right], \\\hat{y_{0}}=\hat{\alpha}+\hat{\beta}x_{0} &\sim& N\left[y_{0},\sigma^{2}\left(\frac{1}{n}+\frac{(x_{0}-\bar{x})^{2}}{S_{xx}}\right)\right].
\end{eqnarray}
    </div>								
These can be used to make confidence intervals and hypothesis tests about these parameters. You will be expected to do these! Derive the confidence intervals for yourself, and think how you could perform hypothesis tests using these results.

			
    </div>								<div class="subsection">
    													
The value of the error variance 								$\sigma^{2}$								 is usually unknown and must be estimated from the data.

Earlier we stated that the residual sum of squares, 								$SS_{error}$								 may be written
								<div class="displaymath">
       \[SS_{error}=\sum_{i=1}^{n}(y_{i}-\hat{\alpha}-\hat{\beta} x_{i})^{2}=S_{yy}-\frac{S^{2}_{xy}}{S_{xx}}.\]
    </div>								

								<div class="lemma">
		<div class="blockheading"> 
    Lemma&nbsp;56&nbsp;		</div><div class="displaymath">
       \[\frac{SS_{error}}{n-2}\]
    </div> is an unbiased estimator of $\sigma^{2}$.
</div>								

It follows from earlier results that if the errors are normally distributed then the random variables
								<div class="displaymath">
       \[\frac{\hat{\beta}-\beta}{\sqrt{\frac{\sigma^{2}}{S_{xx}}}}, \frac{\hat{\alpha}-\alpha}{\sqrt{\sigma^{2}\left(\frac{1}{n}+\frac{\bar{x}^{2}}{S_{xx}}\right)}}, \frac{\hat{y_{0}}-y_{0}}{\sqrt{\sigma^{2}\left(\frac{1}{n}+\frac{(x_{0}-\bar{x})^{2}}{S_{xx}}\right)}}\]
    </div>								
are have the distribution 								$N[0,1].$								

It can be shown that when 								$\sigma^{2}$								 is estimated by 								<div class="displaymath">
       \[\hat{\sigma^{2}}=\frac{SS_{error}}{n-2}\]
    </div>								 then the random variables
								<div class="displaymath">
       \[\frac{\hat{\beta}-\beta}{\sqrt{\frac{\hat{\sigma^{2}}}{S_{xx}}}}, \frac{\hat{\alpha}-\alpha}{\sqrt{\hat{\sigma^{2}}\left(\frac{1}{n}+\frac{\bar{x}^{2}}{S_{xx}}\right)}}, \frac{\hat{y_{0}}-y_{0}}{\sqrt{\hat{\sigma^{2}}\left(\frac{1}{n}+\frac{(x_{0}-\bar{x})^{2}}{S_{xx}}\right)}}\]
    </div>								
all have the Student's t distribution with 								$(n-2)$								 degrees of freedom. Again these can be used to make confidence intervals, or to perform hypothesis tests in the usual way.

			
    </div>								<div class="subsection">
    													<div class="example">
		<div class="blockheading"> 
    Example&nbsp;57&nbsp;		</div>
The following table gives measurements of two variables $x$ and $y$ that are known to be linearly related. The variable $x$ is measured without error, but there is a measurement error associated with $y$ that you can assume is normally distributed with zero mean and variance $\sigma^{2}$.
<div class="center">
       <table class="tabular">				
            <tr class="t">				
        <td class="cL">				
        
	</td>
	</tr><tr class="t">				
        <td class="cL">				
        :  or col1-col2col3-col4 ...
$x$
	</td><td class="cL">				
        	 5.0
	</td><td class="cL">				
        	 7.5
	</td><td class="cL">				
        	10.0
	</td><td class="cL">				
        	12.5
	</td><td class="cLR">				
        	15.0
	</td>
	</tr><tr class="b">				
        <td class="cL">				
        $y$
	</td><td class="cL">				
         	1.23
	</td><td class="cL">				
        	1.39
	</td><td class="cL">				
        	1.52
	</td><td class="cL">				
        	1.66
	</td><td class="cLR">				
        	1.81
	</td>
	</tr>
	</table>
    </div>
Find the least squares estimates of the slope and intercept of the straight line to predict $y$ with $x$. Also calculate an unbiased estimate of the error variance $\sigma^{2}$. Find a $95\%$ confidence interval for the intercept.									
</div>								

											
    </div>			
    </div>			

	

        	<div id="browse-horizontal" style="border-top:1px solid #000;">
        			<p class="left">
    </p>
<p class="right">
            <a href="">1            									<span class="section_title">
						Sampling distributions		</span>							            &nbsp;&rarr;
                                </a>
    </p>
<div style="clear: both;"></div>        	</div>

        	    	
		</div>

    	<div id="content-related", class="sidebar">
        		        </div>
    </div>

    <div id="footer">
            Generated by LatexTree (Cardiff University 2017)
        </div>
</div>

</body>
</html>