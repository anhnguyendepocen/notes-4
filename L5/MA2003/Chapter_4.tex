% !TEX root = main.tex

%------------------------------------------------
\chapter{Cauchy's Theorem}
\section{Cauchy's Theorem for a Triangle}
\begin{definition}
A region $\mathcal{R}$ is called \emph{simply connected} if given any closed contour $C$ in $\mathcal{R}$, all points enclosed by $\mathcal{C}$ also belong to $\mathcal{R}$.
\end{definition}
It is surprisingly difficult to write down the precise definition of a point being enclosed by a contour $\mathcal{C}$, thus we shall treat this notion informally and avoid complicated cases.  Essentially, a simply connected region cannot have any `holes.'

\begin{center}
\hspace{-2cm}
\includegraphics[scale=0.6]{sc_full}
\end{center}
%\begin{comment}
%\begin{center}
%\includegraphics[scale=1]{upperhalf}
%\includegraphics[scale=1]{hplus}
%\vspace*{1cm}
%\includegraphics[scale=1]{sc1}
%\hspace{1cm}
%\includegraphics[scale=1]{sc2}
%\end{center}
%\vspace*{2cm}
%\end{comment}
In the proof of Cauchy's Theorem we will need the following fact about the integral of a continuous function $f$ along a smooth path $\Gamma$: if $\tilde{\Gamma}$ denotes the reverse of $\Gamma$ then
\[
\int_{\tilde{\Gamma}} f = - \int_{\Gamma} f.
\]
We shall also fix the following notation: $T(z_1,z_2,z_3)$ denotes the triangle with vertices $z_1,z_2,z_3$, and hence with edges given by the line segments $[z_1,z_2],[z_2,z_3],[z_3,z_1]$.  The boundary of $T$ is denoted by $\partial T$, which defines a closed contour
\[
\partial T = [z_1,z_2] + [z_2,z_3] + [z_3,z_1].
\]
\begin{theorem}[Cauchy's Theorem for a Triangle]
\label{t:cauchyt} Let $f$ be a function that is holomorphic on a simply connected region $\mathcal{R}$ and let $T(z_1,z_2,z_3)$ be a triangle in $\mathcal{R}$ with boundary $\partial T$.  Then
\[
\int_{\partial T} f = 0.
\]
\end{theorem}
\begin{center}
\includegraphics[scale=0.6]{cauhyt1_full}
\end{center}
Note that if we knew that $f$ were to have an antiderivative $F$ on $\mathcal{R}$, then Theorem~\ref{t:closed} would show that
\[
\int_{\partial T} f =0.
\]
Later we will see that in fact $f$ does have an antiderivative on $\mathcal{R}$, but the proof will rely Cauchy's Theorem for a triangle - thus we need to prove Theorem~\ref{t:cauchyt} without this assumption.
\begin{proof}
We are going to show that given any $\epsilon>0$ we have
\[
\abs{ \int_{\partial T} f } \leq \epsilon [\ell (\partial T)]^2
\]
where $\ell ( \partial T )$ is the length of the boundary of the triangle $T$.

\begin{enumerate}
\item First, we construct a nested sequence of triangles $T \supseteq T_1 \supseteq T_2 \supseteq \ldots \supseteq T_n \supseteq \ldots$, with the property that
\[
\frac{1}{4^n} \abs{ \int_{\partial T} f } \leq \abs{ \int_{\partial T_n} f }
\]
for all $n$.


Using the midpoints of the sides of $T$, construct four similar triangles as shown.

%\vspace*{2cm}
\begin{center}
\includegraphics[scale=0.6]{cauchyt2_full}
\end{center}
%\vspace*{2cm}
Note that with all contours taken anticlockwise,
\[
\int_{\partial T^{(1)}} f + \int_{\partial T^{(2)} } f + \int_{\partial T^{(3)}} f + \int_{\partial T^{(4)}} f = \int_{\partial T} f,
\]
as the integrals along the `interior' edges of the $T^{(j)}$ cancel in pairs\marginpar{This uses $\int_{\tilde{\Gamma}} f = - \int_{\Gamma} f$, where $\tilde{\Gamma}$ is the reverse of $\Gamma$.}:
\begin{center}
\includegraphics[scale=0.6]{cauchyt3_full}
\end{center}

Moreover, by the triangle inequality
\[
\abs{\int_{\partial T} f} \leq \abs{ \int_{\partial T^{(1)}} f} + \abs{\int_{\partial T^{(2)} } f }+ \abs{\int_{\partial T^{(3)}} f }+ \abs{\int_{\partial T^{(4)}} f }. 
\]
%\vspace*{5cm}
At least one of these four integrals has modulus greater than or equal to the other three; call the corresponding triangle $T_1$.  Then
\[
\abs{\int_{\partial T} f} \leq \abs{ \int_{\partial T^{(1)}} f} + \abs{\int_{\partial T^{(2)} } f }+ \abs{\int_{\partial T^{(3)}} f }+ \abs{\int_{\partial T^{(4)}} f } \leq 4 \abs{ \int_{\partial T_1} f},
\]
and so
\[
\frac{1}{4} \abs{ \int_{\partial T } f } \leq \abs{\int_{\partial T_1} f}.
\]
Now, use the midpoints of the edges of $T_1$ to get four more triangles, one of which is a triangle $T_2$ satisfying
\[
\frac{1}{4} \abs{ \int_{\partial T_1} f } \leq \abs{ \int_{\partial T_2} f },
\]
and so
\[
\frac{1}{4^2} \abs{ \int_{\partial T} f } \leq \abs{ \int_{\partial T_2} f }.
\]

\begin{center}
\includegraphics[scale=0.6]{cauchyt4_full}
\end{center}

%\vspace*{5cm}

Continuing in this way we get $T\supseteq T_1 \supseteq T_2 \supseteq \ldots \supseteq T_n \supseteq \ldots$ with 
\[
\frac{1}{4^n} \abs{ \int_{\partial T} f} \leq \abs{ \int_{\partial T_n} f }
\]
for each $n$.
\begin{center}
\includegraphics[scale=0.6]{cauchyt5}
\end{center}

\item There exists a point $z_0 \in \mathcal{R}$ with $z_0 \in T_n$ for all $n$ (as the sequence of triangles is nested).  Let $\epsilon>0$ be given.  We will show that there exists $\delta>0$ such that 
\[
\abs{z-z_0}<\delta \Longrightarrow \abs{f(z)-f(z_0)-(z-z_0)f'(z_0)} \leq \epsilon \abs{z-z_0}.
\]

  Since $f$ is holomorphic on $\mathcal{R}$ it is differentiable at $z_0$ with derivative $f'(z_0)$, or in other words
  \[
  \lim_{h \to 0} \frac{f(z_0+h)-f(z_0)}{h} = f'(z_0).
  \]  

  This means that there is some $\delta>0$ such that
  \begin{align*}
  0 < \abs{h} < \delta &\Longrightarrow \abs{ \frac{f(z_0+h)-f(z_0)}{h} - f'(z_0) } < \epsilon \\
  \shortintertext{ or in other words, writing $z=z_0+h$,}
  0 < \abs{z-z_0} < \delta &\Longrightarrow \abs{ \frac{f(z)-f(z_0)}{z-z_0} - f'(z_0) } < \epsilon \\
  & \Longrightarrow \abs{ \frac{f(z)-f(z_0)-(z-z_0)f'(z_0)}{z-z_0} } < \epsilon.
  \end{align*}
Thus
\[
0 < \abs{z-z_0} < \delta \Longrightarrow \abs{f(z)-f(z_0)-(z-z_0)f'(z_0)} < \epsilon \abs{z-z_0}.
\]
If we allow $z=z_0$, this becomes
\[
\abs{z-z_0} < \delta \Longrightarrow \abs{f(z)-f(z_0)-(z-z_0)f'(z_0)} \leq \epsilon \abs{z-z_0}
\]
(since when $z=z_0$, both sides are zero).

\item With $\epsilon>0$ as before, and $\delta>0$ as chosen in the previous step, we choose $n$ large enough so that $\ell ( \partial T_n ) < \delta$, which can always be done as $\ell ( \partial T_n ) = \frac{1}{2^n} \ell ( \partial T)$ (note that $n$ depends on $\delta$, which in turn depends on $\epsilon$).

We will show that for $z \in \partial T_n$ we have
\[
\abs{f(z)-f(z_0)-(z-z_0)f'(z_0)}<\epsilon \ell ( \partial T_n).
\]

If $z \in \partial T_n$, then the distance from $z$ to $z_0$ can be no more than the length of the longest edge of $T_n$, which is less than $\ell ( \partial T_n)$.
%\vspace*{5cm}
\begin{center}
\includegraphics[scale=0.6]{cauchyt6_full}
\end{center}
%\vspace*{7cm}
Thus
\[
z \in \partial T_n \Longrightarrow \abs{z-z_0} \underbrace{< \ell (\partial T_n) }_{\text{geometry}} \underbrace{< \delta}_{\text{choice of }n},
\]
and hence $T_n \subset D(z_0,\delta)$.

Thus for any $n$ large enough so that $\ell ( \partial T_n) < \delta$, we have
\begin{align*}
z \in \partial T_n \Longrightarrow \abs{f(z)-f(z_0)-(z-z_0)f'(z_0)} &\leq \epsilon \abs{z-z_0} \\
& < \epsilon \ell ( \partial T_n).
\end{align*}
\begin{center}
\includegraphics[scale=1]{cauchyt7}
\end{center}

\item The next step uses the Estimation Lemma to find an upper estimate for $\int_{\partial T_n} f$, where $n$ was chosen in the previous step.  

Note that
\[
\int_{\partial T_n} (\alpha+\beta(z-z_0)) dz =0 \text{ for }\alpha,\beta \in \C,
\]
because $z \mapsto \alpha + \beta (z-z_0)$ has an antiderivative $z \mapsto \alpha z + \frac{\beta(z-z_0)^2}{2}$.


This means that
\[
\int_{\partial T_n} {\Big(} \underbrace{f(z_0)}_{\alpha}+\underbrace{f'(z_0)}_{\beta} (z-z_0) {\Big)} dz =0,
\]
and so
\[
\int_{\partial T_n} f = \int_{\partial T_n} {\Big(} f(z)-\underbrace{f(z_0)-f'(z_0)(z-z_0)}_{\text{integral }0\text{ along } \partial T_n} {\Big)} dz
\]
We shall use the Estimation Lemma to find an upper estimate for this integral.  Indeed, we have
\begin{align*}
\abs{\int_{\partial T_n} f} & = \abs{\int_{\partial T_n} \left[ f(z)-f(z_0)-f'(z_0)(z-z_0) \right]\ dz} &&\\
& \leq \epsilon \ell ( \partial T_n ) \ell ( \partial T_n) &&\text{ By step 3} \\
& = \epsilon \left( \frac{1}{2^n} \ell ( \partial T ) \right) \left( \frac{1}{2^n} \ell ( \partial T) \right) && \text{ as } \ell (\partial T_n) = \frac{1}{2^n} \ell ( \partial T) \\
& = \epsilon \frac{1}{4^n} \left[ \ell ( \partial T) \right]^2.
\end{align*}




%\begin{absolutelynopagebreak}
\item 
At the start, we showed that
\[
\frac{1}{4^n} \abs{ \int_{\partial T} f } \leq \abs{ \int_{\partial T_n} f}.
\]
%\end{absolutelynopagebreak}
By Step 4, we have
\[
\abs{ \int_{\partial T_n} f } \leq \epsilon \frac{1}{4^n} \left[ \ell ( \partial T) \right]^2,
\]
and so
\[
\abs{ \int_{\partial T} f } \leq \epsilon \left[ \ell ( \partial T) \right]^2.
\]
Because $\epsilon>0$ was arbitrary and $\ell ( \partial T)$ is fixed, we get
\[
\abs{ \int_{\partial T} f } = 0, \text{ hence } \int_{\partial T} f =0.
\]
%\vspace*{5cm}
\end{enumerate}
\end{proof}


\begin{example}
Let $f:\C \to \C$ be defined by $f(z)=\sin \left( \exp \left( \cos \left( z^3-4z^2+i \right) \right) \right)$, which is holomorphic on $\C$.  There is no obvious antiderivative for $f$, but nonetheless, Cauchy's Theorem shows us that for any triangle $T$ we have
\[
\int_{\partial T} f = 0.
\]
\end{example}
\begin{example}
Let $f(z) = \dfrac{1}{z}$, which is holomorphic on $\C \backslash \set{0}$.  We shall evaluate $\int_{\partial T} f$ for different triangles $T$.

Let $T_1$ be a triangle enclosing the origin as shown.
\begin{center}
\includegraphics[scale=0.6]{pplane1_full}
\end{center}
Since $\C \backslash \set{0}$ is not simply connected, Cauchy's Theorem for a triangle tells us nothing about
\[
\int_{T_1} \frac{1}{z}\ dz.
\]
However, $\C \backslash \set{0}$ has simply connected sugregions, for example, the left-half plane
\[
\mathcal{L} = \set{ z \in \C : \Re (z) < 0 }.
\]
\begin{center}
\includegraphics[scale=0.6]{pplane2_full}
\end{center}
If $T_2$ is a triangle contained in $\mathcal{L}$, Cauchy's Theorem for a triangle gives
\[
\int_{T_2} \frac{1}{z}\ dz =0.
\]
In fact, if $T$ is any triangle in $\C \backslash \set{0}$ that does not enclose the origin, we can find a simply connected subregion of $\C \backslash \set{0}$ that contains $T$, and conclude that
\[
\int_{T} \frac{1}{z}\ dz =0.
\]



%\vspace*{3cm}

\end{example}
\newpage
%\begin{absolutelynopagebreak}
\subsection{Cauchy's Theorem for Starlit Regions}

\begin{definition}
A region $\mathcal{R}$ is called \emph{starlit} if there is a point $z_{\ast} \in \mathcal{R}$ such that for any $z \in \mathcal{R}$ the line segment $[z_{\ast},z]$ lies inside $\mathcal{R}$.  The point $z_{\ast}$ is called a \emph{star centre} for $\mathcal{R}$.
\end{definition}

The name starlit comes from the idea that the `rays of light' from the `star' $z_{\ast}$ fall on every point of $\mathcal{R}$.  
\begin{note}
\begin{center}
%\includegraphics[scale=0.75]{ellipse} \\
\vspace*{3cm}
\includegraphics[scale=0.75]{annulus}
\end{center}
\vspace*{3cm}
%\end{absolutelynopagebreak}
\end{note}

\begin{center}
\includegraphics[scale=0.6]{starlit_full}
\end{center}

%\begin{comment}
%\begin{example}
%\begin{itemize}
%\item The open disc $D(w,r)$:
%\vspace*{5cm}
%\item The punctured disc $D'(w,r)$:
%\vspace*{5cm}
%\item The region $\C_{\pi} = \set{z \in \C : z \neq 0 \text{ and } \arg (z) \neq \pi }$.
%\vspace*{5cm}
%\end{itemize}
%\end{example}
%\end{comment}
\begin{theorem}
\label{t:starlit}
Let $f$ be holomorphic on a starlit region $\mathcal{R}$ with star centre $z_{\ast} \in \mathcal{R}$.  Then the function $F:\mathcal{R} \to \C$ defined by
\[
F(z) = \int_{[z_{\ast},z]} f
\]
is an antiderivative for $f$ on $\mathcal{R}$.
\end{theorem}
\begin{center}
\includegraphics[scale=0.4]{starlit_antid_full}
\end{center}

\begin{proof}
We need to show that for any $z_0 \in \mathcal{R}$, $F'(z_0)$ exists and is equal to $f(z_0)$, or in other words
\[
\lim_{h \to 0} \frac{F(z_0+h)-F(z_0)}{h} = f(z_0).
\]
  Write
\[
\frac{F(z_0+h)-F(z_0)}{h} = \frac{1}{h} \int_{[z_{\ast},z_0+h]} f - \frac{1}{h} \int_{[z_{\ast},z_0]} f.
\]
For sufficiently small $h$, the triangle $T(z_{\ast},z_0+h,z_0)$ is contained in $\mathcal{R}$ (proof omitted).
\begin{center}
\includegraphics[scale=0.4]{starlit_triangle_full}
\end{center}

  Cauchy's Theorem for a Triangle gives
\[
\int_{[z_*,z_0+h]} f + \int_{[z_0+h,z_0]} f + \int_{[z_0,z_*]} f = 0,
\]
hence
\[
\int_{[z_*,z_0+h]}f - \int_{[z_*,z_0]}f = - \int_{[z_0+h,z_0]} f = \int_{[z_0,z_0+h]} f.
\]
This gives
\[
\frac{F(z_0+h)-F(z_0)}{h} = \frac{1}{h} \int_{[z_0,z_0+h]} f.
\]
We now use the fact that
\[
\int_{[z_0,z_0+h]} 1 dz = h,
\]
which implies that
\[
f(z_0) = f(z_0) \cdot \frac{1}{h} \cdot h = f(z_0) \frac{1}{h} \int_{[z_0,z_0+h]} 1 dz = \frac{1}{h} \int_{[z_0,z_0+h]} f(z_0)\ dz
\]
(since $f(z_0)$ is a constant).  Combining this with the previous step, we see that 
%\newpage
%\vspace*{5cm}
%\newpage
\begin{align*}
 \frac{F(z_0+h)-F(z_0)}{h}-f(z_0)  &= \frac{1}{h} \left[ \int_{[z_0,z_0+h]} f(z)\ dz - \int_{[z_0,z_0+h]} f(z_0)\ dz \right] \\
& = \frac{1}{h} \int_{[z_0,z_0+h]} (f(z)-f(z_0))\ dz 
\end{align*}
(since both integrals are along the same path, we can combine them).
%\begin{absolutelynopagebreak}
Hence
\[
\abs{ \frac{F(z_0+h)-F(z_0)}{h}-f(z_0) } = \frac{1}{\abs{h}} \abs{ \int_{[z_0,z_0+h]} (f(z)-f(z_0)) dz }
\]
Now we shall use the Estimation Lemma to obtain an upper estimate for this integral.  It is easy to see that $\ell([z_0,z_0+h]) = \abs{h}$, thus we must find an upper bound for $\abs{f(z)-f(z_0)}$ when $z \in [z_0,z_0+h]$.
%\end{absolutelynopagebreak}


We use the fact that $f$ holomorphic on $\mathcal{R}$ implies $f$ continuous on $\mathcal{R}$, and in particular, continuous at $z_0$.  Hence given any $\epsilon >0$ there is some $\delta >0$ such that
\[
0< \abs{z-z_0} < \delta \Longrightarrow \abs{f(z)-f(z_0)} < \epsilon.
\]
Thus if $\abs{h} < \delta$ then
\begin{align*}
z \in [z_0,z_0+h] & \Longrightarrow \abs{z-z_0} < \abs{h} < \delta \\
& \Longrightarrow \abs{f(z)-f(z_0)} < \epsilon.
\end{align*}
%\newpage
%\vspace*{25cm}
In other words, once $\abs{h}<\delta$, $\epsilon$ is an upper bound for $\abs{f(z)-f(z_0)}$ for $z \in [z_0,z_0+h]$.  Thus the Estimation Lemma gives the upper estimate
\[
\abs{ \int_{[z_0,z_0+h]} \left( f(z)-f(z_0) \right)\ dz } \leq \epsilon \abs{h}
\]
whenever $\abs{h} < \delta$.

Thus given any $\epsilon>0$ there is $\delta>0$ such that
\begin{align*}
0 < \abs{h} < \delta \Longrightarrow 
\abs{ \frac{F(z_0+h)-F(z_0)}{h}-f(z_0) } & = \frac{1}{\abs{h}} \abs{ \int_{[z_0,z_0+h]} (f(z)-f(z_0) dz } \\
& \leq \frac{1}{\abs{h}} \epsilon \abs{h} = \epsilon.
\end{align*}
But this is equivalent to saying
\[
\lim_{h \to 0} \frac{F(z_0+h)-F(z_0)}{h} = f(z_0),
\]
or in other words, $F'(z_0)=f(z_0)$, as required.
%\vspace*{8cm}

\end{proof}
\begin{theorem}[Cauchy's Theorem for Starlit Regions]
\label{t:cauchyst}
Let $f$ be a function that is holomorphic in a starlit region $\mathcal{R}$ and let $\mathcal{C}$ be a closed contour in $\mathcal{R}$.  Then
\[
\int_{\mathcal{C}} f = 0.
\]
\end{theorem}
\begin{proof}
By Theorem~\ref{t:starlit}, $f$ has an antiderivative in $\mathcal{R}$.  Thus by Theorem~\ref{t:closed}
\[
\int_{\mathcal{C}} f = 0.
\]
\end{proof}
In fact, Cauchy's Theorem extends to more general regions.  We shall not prove the following version of Cauchy's Theorem, but we will sketch the idea later, and use it in subsequent sections.

\begin{theorem}[Cauchy's Theorem for Simply Connected Regions]
\label{t:cauchysc}
Let $f$ be a function that is holomorphic in a simply connected region $\mathcal{R}$, and let $\mathcal{C}$ be a closed contour in $\mathcal{R}$. Then
\[
\int_{\mathcal{C}} f = 0.
\]
\end{theorem}

\subsection{The Complex Logarithm Function}

In this section we will define the Complex Logarithm and Power functions.

%\begin{comment}
%In real analysis, we know that the functions $x \mapsto x^m$, where $m \in \mathbb{Z}$, are easy to integrate, except for the case of $m=-1$.  Indeed, the function 
%\[
%f(x) = \frac{1}{x}
%\]
%does not have an antiderivative of the form $x \mapsto ax^m$ for $a \in \mathbb{R}, m \in \mathbb{Z}$. 
%\end{comment}
Consider the Real function $f:(0,+\infty) \to \R$ defined by $f(x) = \dfrac{1}{x}$. Since $f$ is continuous on $(0,+\infty)$, the Fundamental Theorem of Calculus ensures that $f$ has an antiderivative on $(0,+\infty)$.  One way of defining the \emph{natural logarithm} function, is to define it as an antiderivative of $\dfrac{1}{x}$ on $(0,+\infty)$:
\begin{equation}
\label{e:reallog}
\log (x) = \int_1^x \frac{1}{t}\ dt \quad \text{ for all } x \in (0,+\infty).
\end{equation}
\begin{note}
\begin{enumerate}
\item[(i)] There are of course other ways of defining $\log (x)$ for $x \in (0,+\infty)$, most commonly as the inverse of the exponential function $\exp:\mathbb{R} \to (0,+\infty)$.  This definition is equivalent.
\item[(ii)]  The Fundamental Theorem of calculus actually tells us that for \emph{any} choice of $a \in (0,+\infty)$, the function $F: (0,+\infty) \to \mathbb{R}$ defined by
\[
F(x) = \int_a^x \frac{1}{t}\ dt
\]
is an antiderivative for $x \mapsto \frac{1}{x}$.  However, we must take $a=1$ to ensure that $F$ is indeed the inverse of $x \mapsto e^x$: since $e^0=1$ we want $\log (1)=0$.
\end{enumerate}
\end{note}
We shall generalise~\eqref{e:reallog} to define the Complex Logarithm function, using Theorem~\ref{t:starlit} (the existence of antiderivatives on starlit regions).  The function
\[
f(z) = \frac{1}{z}\quad (z \in \C \backslash \set{0})
\]
is holomorphic on $\C \backslash \set{0}$. However, this set is neither simply connected nor starlit.  Therefore, we will restrict our attention to the subset\marginpar{When we defined $\C_{\pi}$ on Exercise sheet 1, we did not specify whether or not $0$ was included (since $\Arg (0)$ is not defined).  While it was not important at that time, it is now.}
\[
\C_{\pi} = \set{ z \in \C: z \neq 0 \text{ and } \Arg(z) \neq \pi },
\]
which is both starlit and simply connected. 
\begin{center}
\includegraphics[scale=1]{cpi2}
\end{center}
 The point $1$ on the real axis is a star centre for $\C_{\pi}$ - in particular, this allows us to apply Theorem~\ref{t:starlit} to conclude that
\[
z \mapsto \int_{[1,z]} \frac{1}{\zeta}\ d \zeta
\]
is an antiderivative for $z \mapsto \frac{1}{z}$ on $\C_{\pi}$.  We use the variable $\zeta$ (zeta) as the variable of integration because $z$ has been used already.

\begin{definition}
The \emph{Complex Logarithm} function $\Log : \C_{\pi} \to \C$ is defined by
\[
\Log (z) = \int_{[1,z]} \frac{1}{\zeta}\ d \zeta\quad \text{ for } z \in \C_{\pi}.
\]
It is holomorphic on $\C_{\pi}$.
\end{definition}
In other words, to find the value of $\Log (z)$ we need to evaluate the integral
\[
\int_{[1,z]} \frac{1}{\zeta}\ d \zeta.
\]
We can do this directly by choosing a parameterisation of the path $[1,z]$, however, it is much easier to obtain an alternative definition using the Contour Independence Theorem~\ref{t:contint}, which allows us to integrate along any contour from $1$ to $z$ in $\C_{\pi}$.
\begin{center}
\includegraphics[scale=0.4]{logpath_full}
\end{center}
We shall use the contour $\mathcal{C}=\Gamma_1+\Gamma_2$, where $\Gamma_1=[1,\abs{z}]$ is the straight line segment along the real axis from $1$ to $\abs{z}$, and $\Gamma_2$ is the arc of the circle with centre $0$ and radius $\abs{z}$ from the point $\abs{z}$ on the positive real axis to $z$. Parameterise $\Gamma_1$ with $\gamma_1:[1,\abs{z}] \to \C$, $\gamma_1(t)=t$, so that $\gamma_1'(t)=1$.

 In order to ensure that we stay within $\C_{\pi}$, we need to take care that $\Gamma_2$ is traversed
\begin{itemize}
\item anticlockwise if $\Arg (z) \geq 0$, and
\item clockwise if $\Arg(z)<0$.
\end{itemize}
To do this, we parameterise $\Gamma_2$ with $\gamma_2:[0, \abs{\Arg(z)}] \to \C$, 
\[\gamma_2(t) = \begin{cases}
\abs{z} \left( \cos (t) + i \sin (t) \right) & \text{ if } \Arg(z) \geq 0 \\
\abs{z} \left( \cos (-t) + i \sin (-t) \right) & \text{ if } \Arg(z)<0,
\end{cases}
\]
and thus
\[
\gamma_2'(t) = \begin{cases}
 i \gamma_2 (t) & \text{ if } \Arg(z) \geq 0 \\
 -i \gamma_2 (t) & \text{ if } \Arg (z)<0.
\end{cases}
\]
Hence
\begin{align*}
\Log (z) & = \int_{\Gamma_1} \frac{1}{\zeta}\ d\zeta + \int_{\Gamma_2} \frac{1}{\zeta}\ d\zeta \\
& = 
\int_{1}^{\abs{z}} \frac{1}{t}\ dt + \int_0^{\abs{\Arg (z)}} \frac{1}{\gamma_2(t)} \gamma_2'(t)\ dt \\
& = \left[ \log \abs{t} \right]_1^{\abs{z}} + 
\begin{cases}
\int_0^{\Arg (z)} i\ dt & \text{ if } \Arg (z) \geq 0 \\
\int_0^{-\Arg(z)} -i\ dt & \text{ if } \Arg (z) < 0 
\end{cases}
\\
& = \log \abs{z} + i \Arg (z).
\end{align*}

%\vspace*{20cm}
This yields the following alternative definition of the Complex Logarithm function:
\begin{equation}
\Log (z) = \log ( \abs{z} )+i \Arg (z) \quad (z \in \C_{\pi}).
\end{equation}
\begin{note}
This definition of $\Log (z)$ uses the principal value of the argument - that is to say, our definition depends on us taking $\arg (z) \in (-\pi, \pi]$.  For this reason it is sometimes called the \emph{principal branch} of the (complex) Logarithm function.  Other `branches' (i.e. other definitions) of $\Log (z)$ are possible by taking different values of the argument.
\end{note}
\begin{example}
\begin{align*}
\Log (7) & = \log \abs{7} + i \Arg (z) = \log(7) \\
\Log (-2i) & = \log \abs{-2i} + i \Arg (-2i) \\
& = \log(2) -i \frac{\pi}{2} \\
\Log (1+i ) & = \log \abs{1+i} + i \Arg (1+i) \\
& = \log ( \sqrt{2} ) + i \frac{\pi}{4} \\
& = \frac{1}{2} \log (2) + i \frac{\pi}{4.}
\end{align*}
\end{example}

So far, $\Log (z)$ is defined (and is holomorphic) on $\C_{\pi}$. We can extend the domain of $\Log$ to $\C \backslash\set{0}$ by defining it on the negative real axis, where points have principal argument $\pi$, as follows:
\[
\Log (z) = \log \left( \abs{z} \right) + i \pi, \mbox{ for $z \neq 0$ on the negative real axis.}
\]
\begin{questions}
\question
Is $\Log(z)$ continuous on $\C \backslash \set{0}$?
\end{questions}

\begin{center}
\includegraphics[scale=0.4]{log_discontinuous_full}
\end{center}
%\vspace*{5cm}
For $t$ on the negative real axis,
\[
\rlim{z \to t}{ \Im (z) > 0 } \Arg (z) = \pi, \text{ while} \rlim{z \to t}{\Im(z)<0} \Arg (z) = -\pi.
\]
Thus $z \mapsto \Arg (z)$ is discontinuous on the negative real axis.  Since $\Arg (z)$ is the imaginary part of $\Log(z)$, it follows that $\Log$ is also discontinuous on the negative real axis.

\bigskip

Recall that the complex exponential function $\exp: \C \to \C$ is defined as
\begin{equation}
\label{e:exp}
\exp(x+iy) = e^x \left( \cos (y) + i \sin (y) \right),
\end{equation}
and $\exp$ satisfies
\begin{align}
& \exp' (z) =\exp (z) \quad (z \in \C) \\
& \exp (z_1+z_2) = \exp(z_1)+\exp(z_2) \quad (z_1,z_2 \in \C).
\end{align}
\begin{questions}
\question
Is $\Log$ the inverse of $\exp$?
\end{questions}
Not exactly.  For any $z \in \C \backslash \set{0}$, we have
\begin{align*}
\exp ( \Log (z)) & = \exp \left( \log ( \abs{z} ) + i \Arg (z) \right) \\
& = e^{\log \abs{z}} \left( \cos ( \Arg (z))+i \sin ( \Arg (z) ) \right) \\
& = \underbrace{\abs{z}\left( \cos ( \Arg (z))+i \sin ( \Arg (z) ) \right)}_{\text{Polar form of }z} \\
& = z.
\end{align*}
However, $\exp:\C \to \C$ is not injective (e.g. $\exp(0)=\exp(i2\pi)=1$), thus we cannot have $\Log ( \exp(z)) = z$ for all $z$.  We do have
\begin{align*}
\Log ( \exp (x+iy ) ) & = \log \abs{\exp(x+iy)} + i \Arg (\exp(x+iy)) &&\\
& = \log(e^x) + i (y-2k\pi) && \text{ for some }k \in \mathbb{Z} \\
\end{align*}
Hence $\Log (\exp(z))=z$ if and only if $-\pi<\Im(z) \leq \pi$.
%\newpage
%\vspace*{10cm}

% \begin{comment}:  the $n^{th}$ roots of a complex number $z = r ( \cos (\theta) + i \sin (\theta))$ are given by
%\[
%\sqrt[n]{r} \left( \cos \left( \frac{\theta + 2k \pi}{n} \right) + i \sin \left( \frac{\theta + 2k \pi}{n} \right) \right) \text{ for } k=0,1,\ldots n,
%\]
%where $\sqrt[n]{r}$ is the real $n^{th}$ root of $r$.\end{comment} 

\bigskip
We have seen already how to take roots of complex numbers. We will now extend this definition to \emph{complex} powers of complex numbers, allowing us to compute expressions such as
\[
2^{i}, i^{i}, (1+3i)^{1-i}
\]
and so on.  The following observation about powers of real numbers may be useful: suppose we want to find the value of $x^a$ for some $a,x \in \mathbb{R}$ with $x>0$.  We know that
\[
\log (x^a) = a \log (x),
\]
and since $e^{\log(y)}=y$ for all $y \in (0,+\infty)$, we have
\[
x^a = e^{  \log \left( x^a \right) } = e^{  a \log (x) }.
\]
Thus we may take $e^{ a \log (x) }$ to be the definition of $x^a$.
\begin{definition}
\label{d:ppower}
For $\alpha \in \C$, the \emph{Principal $\alpha^{th}$ Power Function} is defined by
\[
z^{\alpha} = \exp \left( \alpha \Log (z) \right)
\]
for all $z \in \C \backslash \set{0}$.
\end{definition}
Because $\Log$ is holomorphic on $\C_{\pi}$ and $\exp$ is holomorphic on $\C$, the Principal Power function is holomorphic on $\C_{\pi}$.  `Principal' refers to the fact that we have used the principal branch of the Logarithm function to define the power function - taking different branches of the Logarithm function will give different power functions.

\begin{example}
\begin{enumerate}
\item[(i)] Let us verify that $i^2=-1$ agrees with Definition~\ref{d:ppower}.

Indeed, we have
\begin{align*}
i^2 = \exp \left( 2 \Log (i) \right) & = \exp \left( 2 \left[ \log \abs{i}+i \Arg(i) \right] \right) \\
& = \exp \left( 2 \left[ \log(1)+i \frac{\pi}{2} \right] \right) \\
& = \exp(i\pi) = -1.
\end{align*}
%\vspace*{7cm}
\item[(ii)] Calculate $2^i$
%\vspace*{7cm}

This time, we have
\begin{align*}
2^i = \exp(i \Log(2) ) & = \exp(i \log (2) ) \\
& = \cos(\log(2))+i \sin(\log(2)).
\end{align*}
\item[(iii)] Calculate $i^i$.

\[
i^i = \exp(i\Log(i)) = \exp (i ( i \frac{\pi}{2})) = \exp(-\frac{\pi}{2}) = e^{-\pi/2}.
\]
\end{enumerate}
\end{example}

%\begin{absolutelynopagebreak}
%\begin{question}[See Exercise Sheet 4]
%Which of the following statements about the Principal Logarithm and Power functions are true or false, where $z,z_1,z_2\in \C \backslash \set{0}$ and $\alpha,\beta \in \C$?  
%\begin{enumerate}
%\item[(i)] $\Log (z_1z_2) = \Log (z_1)+\Log (z_2)$
%\item[(ii)] $z^{\alpha}z^{\beta}=z^{\alpha+\beta}$
%\item[(iii)] $(z^{\alpha})^{\beta} = z^{\alpha\beta}$
%\item[(iv)] $z_1^{\alpha}z_2^{\alpha} = (z_1z_2)^{\alpha}$
%\item[(v)] $\exp (\alpha) = e^{\alpha}$ (i.e. $\exp (\alpha)$, defined via~\eqref{e:exp}, is equal to the principal $\alpha^{th}$ power of the real number $e$).
%\end{enumerate}
%For those that are true, give a proof, and for those that are false, illustrate this with a counterexample.
%\end{question}
%\end{absolutelynopagebreak}

\begin{exercise}[See Exercise Sheet 4]
\begin{questions}
\question
Which of the following statements about the Principal Logarithm and Power functions are true or false, where $z,z_1,z_2\in \C \backslash \set{0}$ and $\alpha,\beta \in \C$?  
\begin{parts}
\part $\Log (z_1z_2) = \Log (z_1)+\Log (z_2)$
\part $z^{\alpha}z^{\beta}=z^{\alpha+\beta}$
\part $(z^{\alpha})^{\beta} = z^{\alpha\beta}$
\part $z_1^{\alpha}z_2^{\alpha} = (z_1z_2)^{\alpha}$
\part $\exp (\alpha) = e^{\alpha}$ (i.e. $\exp (\alpha)$, defined via~\eqref{e:exp}, is equal to the principal $\alpha^{th}$ power of the real number $e$).
\end{parts}
For those that are true, give a proof, and for those that are false, illustrate this with a counterexample.
\end{questions}
\end{exercise}
%------------------------------------------------

