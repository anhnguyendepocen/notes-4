% !TEX root = main.tex

%-------------------------------------------------
\section{Elementary probability}\label{ss:elementary}

What is the probability that a random event $A$ occurs? One answer is that this is a number between $0$ and $1$ that indicates how likely $A$ is to occur. Putting aside the question of how we might find such a number, this is not a convincing definition because we must now explain what we mean by ``how likely''. It turns out that probability is difficult to define precisely.

Suppose for the moment however that probabilities have been assigned to the individual outcomes of a random experiment. This can be represented by a \emph{probability mass function},
\[
\begin{array}{cccl}
p:	& \Omega	& \longrightarrow	& [0,1] \\
	& \omega 	& \mapsto			& p(\omega).
\end{array}
\]
Because we want to \emph{quantify} how likely $A$ is to occur, it is reasonable to require that $p(\omega)\geq 0$ for all $\omega\in\Omega$. If the sample space is a countable set, it is also reasonable to define the probability of an event $A$ to be equal the sum of the probabilities of the outcomes it contains:
\begin{equation}\label{eq:prob_event}
P(A) = \sum_{\omega\in A} p(\omega).\tag{*}
\end{equation}
This is a \emph{probability distribution function}, which assigns probabilities to \emph{subsets} of the sample space.

\bigskip
Intuitively, probability should somehow reflect a sense of \emph{proportion}, between how likely $A$ is to occur and how likely $A$ is not to occur. 
\bit
\it The ratio $P(A)/P(A^c)$ is called the \emph{odds on} $A$ occurring.
\it The ratio $P(A^c)/P(A)$ is called the \emph{odds against} $A$ occuring. 
\eit
Odds suffer by the fact that they can take arbitrarily large values. A better definition is obtained by expressing how likely $A$ is to occur as a proportion of how likely \emph{any} event is to occur, and make the latter equal to $1$:
\[
\sum_{\omega\in\Omega} p(\omega) = 1.
\]
%The probability $P(A)$ can then be viewed as a proportion of this total ammount.

\begin{exercise}\label{exe:elementary_properties}
Show that probability distribution functions as defined in \eqref{eq:prob_event} have the following properties, where $A$ and $B$ are subsets of the sample space:
\ben
\it $P(\emptyset)=0$ and $P(\Omega)=1$.
\it Complementarity: $P(A^c) = 1 - P(A)$.
\it Monotonicity: if $A\subseteq B$ then $P(A)\leq P(B)$.
\it Additivity: if $A$ and $B$ are disjoint then $P(A\cup B)=P(A)+P(B)$.
\it Addition formula: $P(A\cup B)=P(A)+P(B)-P(A\cap B)$.
\een
\end{exercise}

%-----------------------------
\subsection{Conditional probability}

Suppose we know that event $B$ occurs. Then event $A$ occurs if and only if $A\cap B$ occurs, and the probability that $A$ occurs should now represent how likely $A\cap B$ is to occur as a proportion of how likely $B$ was to occur in the first place. Thus we define the \emph{conditional probability of $A$ given $B$} to be
\[
P(A|B) = \frac{P(A\cap B)}{P(B)}.
\] 
provided that $P(B)>0$. 

To analyse random events it is often useful to break then into disjoint components and analyse each component separately.
\begin{definition}
A family of sets $\{A_1,A_2,\ldots\,A_n\}$ is called a \emph{partition} of set $B$ if 
\ben
\it $B \subseteq \bigcup_{i=1}^{n} A_i$ and
\it $A_i\cap A_j = \emptyset$ for all $i\neq j$.
\een
\end{definition}

% partition theorem
\begin{theorem}[Partition Theorem]\label{thm:partition}
If $\{A_1,A_2,\ldots,A_n\}$ is a partition of $B$ then 
$
P(B) = \sum_{i=1}^{n} P(B|A_i)P(A_i).
$
\begin{proof}
Because the $A_i$ are disjoint and $B$ is contained in $\subseteq\cup_{i=1}^{n}A_i$, we can represent $B$ as a disjoint union,
\[
B = (B\cap A_1)\cup (B\cap A_2)\cup \ldots = \bigcup_{i=1}^{\infty}(B\cap A_i)
\]
By the additivity property (see Exercise~\ref{exe:elementary_properties})
\begin{align*}
P(B)
	= P\left(\bigcup_{i=1}^{\infty}(B\cap A_i)\right)
	= \sum_{i=1}^{\infty}P(B\cap A_i)
	= \sum_{i=1}^{\infty}P(B|A_i)P(A_i),
\end{align*}
as required.
\end{proof}
\end{theorem}

If we perform a random experiment and observe that $B$ occurs, how does this change what we know about event $A_i$? This is answered by Bayes' theorem.

\begin{theorem}[Bayes' Theorem]\label{thm:bayes}
If $\{A_1,A_2,\ldots,A_n\}$ is a partition of $B$ and $P(B)>0$, then
$
P(A_i|B) = \displaystyle\frac{P(B|A_i)P(A_i)}{\sum_j P(B|A_j)P(A_j)}.
$
\begin{proof}
Set intersection is commutative so
\[
P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{P(B\cap A)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}.
\]
Hence,
\[
P(A_i|B) 
	= \frac{P(B|A_i)P(A_i)}{P(B)}
	= \frac{P(B|A_i)P(A_i)}{\sum_{j=1}^{\infty} P(B|A_j)P(A_j)}
\]
where the last equality follows by the partition theorem, as required.
\end{proof}
\end{theorem}

% elementary example
\begin{example}
One in every $100$ people has a certain disease. People having the disease test positively with probability 0.92; people not having the disease test negatively with probability 0.97. If a person tests positively for the disease, what is the probability that the person actually has the disease? 
\begin{solution}
Let $A$ be the event that the person has the disease. Let $B$ be the event that
the person tests positively: 
\[
P(A)=0.01,\ P(A^c)=0.99,\ P(B|A)=0.92 \text{ and } P(B|A^c)=0.03
\]
The set $\{A,A^c\}$ is a partition of $B$, so by the partition theorem,
\[
P(B) 
	= P(B|A)P(A)+P(B|A^c)P(A^c) 
	= (0.92\times 0.01)+(0.03\times 0.99)
	= 0.029403.
\]
By Bayes' theorem,
\[
P(A|B)	
	= \frac{P(B|A)P(A)}{P(B)}
	= \frac{0.92\times 0.01}{0.029403}
	= 0.3129 \text{ approx.}
\]
Approximately one third of positive tests are ``true positives'', the remainder are ``false positives''.
\end{solution}
\end{example}

\begin{exercise}
\begin{questions}
\question % Bertrand
\textbf{Bertrand's box paradox}. Suppose we have a box containing two gold coins, a box containing two silver coins, and a box containing one gold coin and one silver coin. A box is chosen at random, and a coin is chosen at random from the box. If the chosen coin is a gold coin, what is the probability that the box contains another gold coin?
\begin{answer}
Let $A$ be the event that a gold coin is chosen. By Bayes theorem (with the obvious notation),
\begin{align*}
P(GG|A) 
	& = \frac{P(A|GG)P(GG)}{P(A|GG)P(GG)+P(A|SS)P(SS)+P(A|GS)P(GS)} \\
	& = \frac{1\times 1/3}{(1\times 1/3)+(0\times 1/3)+(1/2\times 1/3)}
	= \frac{2}{3}.
\end{align*}
This is related to the famous \emph{Monty Hall problem}. 
\end{answer}
\question % Galton
\textbf{Galton's paradox}. Three fair coins are tossed independently. Given that at least two are alike, the probability that they are all alike is $1/2$. Do you agree?
\begin{answer}
No. For any outcome at least two coins will be alike, so this provides no additional information. In fact,
\[
P(\text{All are alike} | \text{Two are alike}) = P(\text{All are alike}) = 1/4.
\]
\end{answer}
\end{questions}
\end{exercise}


%-----------------------------
\subsection{Independence}

If the probability that event $A$ occurs is not affected by whether or not event $B$ occurs (and vice versa), we say that $A$ and $B$ are \emph{independent}. 

% definition: independence
\begin{definition}
Two events $A$ and $B$ are said to be \emph{independent} if $P(A|B)=P(A)$ or equivalently
\[
P(A\cap B) = P(A)P(B).
\]
\end{definition}

\begin{example}
A fair die is rolled once. Let $A$ be the event that the outcome is even, and let $B$ be the event that the outcome is divisible by $3$. Are $A$ and $B$ independent?
\begin{solution}
\[
P(A) = P(\{2,4,6\}) = 1/2;\quad
P(B) = P(\{3,6\}) = 1/3;\quad
P(A\cap B) = P(\{6\}) = 1/6.
\]
Thus $A$ and $B$ are independent because $P(A\cap B) = P(A)P(B)$.
\end{solution}
\end{example}

%-----------------------------
%\subsubsection{Pairwise and total independence}
The notion of independence can be extended to three or more events.
 
\begin{definition}
A family of events $\{A_1,A_2,\ldots\}$ is said to be 
\ben
\it \emph{pairwise independent} if $P(A_i\cap A_j)=P(A_i)P(A_j)$ for all $i\neq j$.
\it \emph{totally independent} if for every finite sub-family 
$\{B_1,B_2,\ldots,B_m\}\subset \{A_1,A_2,\ldots\}$, 
\[
P(B_1\cap B_2\cap \ldots \cap B_m) = P(B_1)P(B_2)\cdots P(B_m).
\]
\een
\end{definition}
Note that total independence implies pairwise independence, but not vice versa.

\begin{example}
Let $\Omega=\{1,2,3,4\}$ where each outcome is equally likely, and consider the events $A=\{1,2\}$, $B=\{1,3\}$ and $C=\{1,4\}$. Show that $\{A,B,C\}$ is pairwise independent but not totally independent.
\begin{solution}
\bit
\it
$A$ and $B$ are independent: $P(A)=P(B)=1/2$ and $P(A\cap B)=1/4$ so $P(A\cap B) = P(A)P(B)$. 
\it
Similary $A$ and $C$ are independent, and $B$ and $C$ are indpendent, so the set $\{A,B,C\}$ is pairwise independent. 
\it
In contrast, $P(A\cap B\cap C) = 1/4$ but $P(A)P(B)P(C)=1/8$, so the set $\{A,B,C\}$ is not totally independent.
\eit
\end{solution}
\end{example}

\begin{exercise}
\begin{questions}
\question % (1)
Let $A$ and $B$ be two events with $P(A)=0.2$, $P(B)=0.5$ and $P(A\cap B)=0.1$. Compute the conditional probability $P(A|B)$ and decide whether or not $A$ and $B$ are independent.
\begin{answer}
\bit
\it $P(A|B)   = P(A\cap B)/P(B) = 0.1/0.5 = 0.25$.
\it $P(A|B)=P(A)$ so $A$ and $B$ are independent.
\eit
\end{answer}

\question % (2)
Show that if $A$ and $B$ are independent then $A$ and $B^c$ are also independent.
\begin{answer}
\begin{align*}
P(A\cap B^c) 
	& = P(A) - P(A\cap B) \\
	& = P(A) - P(A)P(B) \quad\text{by independence,} \\
	& = P(A)(1-P(B)) \\
	& = P(A)P(B^c).
\end{align*}
\end{answer}

\question % de Mere
\textbf{de M\'{e}r\'{e}'s paradox}. Solve de M\'{e}r\'{e}'s paradox by computing the probability that at least one six appears in 4 rolls of a single fair die, and the probability that at least one double-six appears in 24 rolls of two fair dice. 
\begin{answer}
Assuming that the rolls are independent:
\begin{align*}
P(\text{at least one six in 4 rolls of a single die)}
	& = 1- P(\text{no sixes obtained in 4 rolls}) \\
	& = 1 - (5/6)^4 \quad\text{(by independence)} \\
	& = 0.5177 \\
P(\text{at least one double six in 24 rolls of two dice}) 
	& = 1- P(\text{no double-sixes in 24 rolls}) \\
	& = 1 - (35/36)^{24} \quad\text{(by independence)} \\
	& = 0.4914
\end{align*}
\end{answer}

\question % independence
Two fair dice are rolled independently. Let $A$ be the event that the first die shows $3$, $B$ that the second die shows $4$ and and $C$ that the total shown on the two dice is $7$.
\begin{parts}
\part
Define a sample space for the experiment and identify the subsets corresponding to events $A$, $B$ and $C$.
\begin{answer}
\begin{itemize}
\item[] $\Omega = \{(i,j):1\leq i,j\leq 6\}$
\item[] $A = \{(3,j):1\leq j\leq 6\}$
\item[] $B = \{(i,4):1\leq i\leq 6\}$
\item[] $C = \{(i,j):1\leq i,j\leq 6\mbox{ and }i+j=7\}$
\end{itemize}
Note that because each outcome is equally likely, $P(A)=P(B)=P(C)=1/6$.
\end{answer}
\part
Show that $\{A,B,C\}$ is pairwise independent but not (totally) independent.
\begin{answer}
To show that the set $\{A,B,C\}$ is pairwise independent, we need to show that any two events chosen from the set are independent of each other.  
\bit
\it For $A$ and $B$, their intersection $A\cap B$ is the event $\{(3,4)\}$, consisting of the single outcome $(3,4)$. This means that $P(A\cap B)=1/36$ and hence $P(A\cap B) = P(A)P(B)$. 
\it Similarly, $A\cap C=\{(3,4)\}$ so $P(A\cap C)=P(A)P(C)$, and $B\cap C=\{(3,4)\}$ so $P(B\cap C)=P(B)P(C)$.  Hence, the set $\{A,B,C\}$ is pairwise independent.  
\it However, the intersection of all three events is also $\{(3,4)\}$ so $P(A\cap B\cap C)=1/36$. Hence $P(A\cap B\cap C) \neq P(A)P(B)P(C)$ so the set $\{A,B,C\}$ is not totally independent.
\eit
\end{answer}
\end{parts}

\question % bayes theorem (GS 1.8.15)
A random number $N$ of dice are rolled independently. Suppose that $P(N=k) = 2^{-k}$ for $k\in\{1,2,\ldots\}$ (and zero otherwise). Let $S$ be the sum of the scores shown on the dice. Show that
\begin{parts}
\part $P(N=2|S=4)=432/2197$,
\begin{answer}
The event $S=4$ can only occur when
\bit
\it $N=1$ and the die shows $4$ (which occurs with probability $1/6$),
\it $N=2$ and the dice show $(1,3)$, $(2,2)$ or $(3,1)$ (which occurs with probability $3/36$),
\it $N=3$ and the dice show $(1,1,2)$, $(1,2,1)$ or $(2,1,1)$ (which occurs with probability $3/216$),
\it $N=4$ and all four dice show $1$ (which occurs with probability $1/1296$).
\eit
By Bayes' theorem,
\begin{align*}
P(N=2|S=4)
	& = \frac{P(S=4|N=2)P(N=2)}{\sum_{k=1}^{\infty} P(S=4|N=k)P(N=k)} \\
	& = \frac{3/36\times 1/4}{(1/6\times 1/2) + (3/36\times 1/4) + (3/216\times 1/8) + (1/1296\times 1/16)} \\
	& = \frac{432}{1728 + 432 + 36 + 1}
	  = \frac{432}{2197}.
\end{align*}
\end{answer}

\part $P(S=4|N\text{ is even})=433/6912$,
\begin{answer}
Using the formula for a geometric series with first term $1/4$ and common raio $1/4$, we have
\[
P(\text{$N$ even}) = \frac{1}{4} + \frac{1}{16} + \frac{1}{64} + \ldots = \frac{1}{3}.
\]
If $N$ is even, then $S=4$ only if 
\bit
\it $N=2$ and the dice show $(1,3)$, $(2,2)$ or $(3,1)$ (which occurs with probability $3/36$),
\it $N=4$ and all four dice show $1$ (which occurs with probability $1/1296$).
\eit
Hence,
\begin{align*}
P(S=4|\text{$N$ even})
	& = \frac{P(\text{$S=4$ and $N$ even})}{P(\text{$N$ even})} \\ 
	& = \frac{P(S=4|N=2)P(N=2) + P(S=4|N=4)P(N=4)}{P(\text{$N$ even})} \\ 
	& = \frac{(3/36\times 1/4) + (1/1296\times 1/16)}{1/3} 
	= \frac{433}{6912}.
\end{align*}
\end{answer}

\part $P(N=2|$S=4\text{ and the first die shows $1$}) = 144/169,
\begin{answer}
Let $D$ be the score on the first die. If $D=1$, then $S=4$ only if 
\bit
\it $N=2$ and the other die shows $3$ (which occurs with probability 1/6),
\it $N=3$ and the other two dice show $(1,2)$ or $(2,1)$ (which occurs with probability 2/36), or
\it $N=4$ and the other three dice all show $1$ (which occurs with probability 1/216).
\eit
Hence,
\begin{align*}
P(N=2|S=2,D=1)
	& = \frac{P(N=2,S=4,D=1)}{P(S=4,D=1)} \\ 
	& = \frac{1/6\times 1/4}{(1/6\times 1/4) +(2/36\times 1/8) + (1/216\times 1/16)}
	= \frac{144}{169}.
\end{align*}
\end{answer}

\part $P(\text{Largest number is $m$}) = m/(12-m)$ for every $m\in\{1,2,3,4,5,6\}$.
\begin{answer}
Let $M$ be the maximum number shown on the dice. For $m\in\{1,2,3,4,5,6\}$, the probability that $m$ is the maximum number shown on a single die is $m/6$. Assuming that the dice are independent, the probability that $m$ is the maximum number shown on $k$ dice is $(m/6)^k$. Hence,
\begin{align*}
P(M\leq m)
	& = \sum_{k=1}^{\infty}P(M\leq m|N=k)P(N=k) \\
	& = \sum_{k=1}^{\infty}\left(\frac{m}{6}\right)^k\frac{1}{2^k} 
	= \sum_{k=1}^{\infty}\left(\frac{m}{12}\right)^k 
	= \frac{m}{12-m},
\end{align*}
where we have used the formula for a geometric series whose first term and common ratio are both equal to $m/12$.
\end{answer}
\end{parts}

\end{questions}
\end{exercise}



