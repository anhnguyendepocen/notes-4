% !TEX root = main.tex

%-------------------------------------------------
\section{The Markov and Chebyshev inequalities}\label{sec:conc}

%-----------------------------
\subsection{Markov's inequality}

If the distribution of a random variable is not known, probabilities can be estimated using the moments of the distribution. A simple upper bound on the tail probabilities of non-negative random variables is provided by \emph{Markov's inequality}.

% theorem: markov inequality
\begin{theorem}[Markov's inequality]
Let $X$ be a non-negative random variable. Then for every $a>0$,
\[
\prob(X\geq a) \leq \frac{\expe(X)}{a}.
\]
\end{theorem}

% proof
\begin{proof}
Let $I_A$ be the indicator function of event $A = \{\omega: X(\omega)\geq a\}$. 
\[
I_A(\omega) = \left\{\begin{array}{ll} 0 & \text{if}\ X(\omega) < a, \\ 1 & \text{if}\ X(\omega)\geq a. \end{array}\right.
\]
\bit
\it If $\omega\in A$ then $X(\omega)   \geq a = a I_A(\omega)$.
\it If $\omega\notin A$ then $X(\omega)\geq 0 = a I_A(\omega)$.
\eit
In either case we have $X(\omega)\geq a I_A(\omega)$ so by the monotonicity of expectation,
\[
\expe(X) \geq a\expe(I_A) = a\prob(A) \equiv a\prob(X\geq a).
\]
Hence $\prob(X\geq a) \leq \expe(X)/a$ as required.
\end{proof}

% example
\begin{example}
A fair die is rolled once. Use Markov's inequality to find an upper bound on the probability that we observe a score of at least $5$.
\end{example}

\begin{solution}
Let $X$ be the number shown on the die. Then $X$ is a non-negative random variable with expectation $\expe(X) = 7/2$.
Markov's inequality yields the upper bound 
\[
\prob(X\geq 5) \leq \frac{\expe(X)}{5} = \frac{7}{10}.
\]
In this example we know that $\prob(X\geq 5)=1/3$, which illustrates that Markov's inequality provides only crude bounds on tail probabilites. Indeed, for the probability $\prob(X\geq 3)$ Markov's inequality yields
\[
\prob(X\geq 3) \leq \frac{\expe(X)}{3} = \frac{7}{6}.
\]
This tells us nothing useful, because we know that $\prob(X\geq 3)\leq 1$. 
\end{solution}
%-----------------------------
\subsection{Chebyshev's inequality}

An upper bound on the absolute deviation of a random variable from its mean is provided by \emph{Chebyshev's inequality}.

\begin{corollary}[Chebyshev's inequality]
Let $X$ be any random varible with finite expectation. Then for all $\epsilon>0$,
\[
\prob(|X-\expe(X)|\geq \epsilon) \leq \frac{\var(X)}{\epsilon^2}.
\]
\end{corollary}
\begin{proof}
Take the non-negative random variable $\big[X-\expe(X)\big]^2$ in Markov's inequality with $a=\epsilon^2$:
\begin{align*}
\prob(|X-\expe(X)|\geq \epsilon) 
	& = \prob\big[\big(X-\expe(X)\big)^2\geq \epsilon^2\big] 
	\leq	\frac{\expe\big[\big(X-\expe(X)\big)^2\big]}{\epsilon^2} 
	= \frac{\var(X)}{\epsilon^2},
\end{align*}	
as required.
\end{proof}

\begin{example}
Suppose that $\expe(X) = 0$ and $\var(X) = 1$. Find an integer value $k$ such that $\prob(|X|\geq k) \leq 0.01$.
\begin{solution}
$X$ is not non-negative, so we can not use Markov's inequality here. By Chebyshev's inequality,
\[
\prob(|X|\geq k) = \prob(|X-\expe X|\geq k) \leq \frac{\var(X)}{k^2} = \frac{1}{k^2} %\leq \frac{1}{100}
\]
so can take any $k\geq 10$.
\end{solution}
\end{example}

% example
\begin{example}
Let $X$ be a continuous random variable with expected value $3.6$ and standard deviation $1.2$. 
\par Show that $\prob(1.2\leq X\leq 6.0) \geq 0.75$.
\begin{solution} % <<<
\par
The event $\{1.2 \leq X \leq 6.0\}$ can be written as $\{|X-3.6| \leq 2.4\}$, and by Chebyshev's inequality,
\[
\prob(|X-3.6|> 2.4) \leq \frac{\var(X)}{2.4^2} = \frac{1.2^2}{2.4^2} = \frac{1}{4},
\]
so $\prob(1.2 \leq X \leq 6.0) \geq 0.75$, as required.
\end{solution}
\end{example}


\begin{exercise}
\begin{questions}

\question
What does Chebyshev's inequality tell us about the probability that the value taken by a random variable deviates from its expectation by five or more standard deviations?
\begin{answer}
For any random variable $X$ with finite variance $\sigma^2$,
\[
\prob(|X-\mu|\geq 5\sigma) \leq \frac{\sigma^2}{(5\sigma)^2} = \frac{1}{25} = 0.04.
\]
Thus for any distribution, we are very unlikely to observe values that are more than 5 standard deviations away from the mean.
\end{answer}

\question
A fair coin is tossed $n$ times. Does Chebyshev's inequality ensure that the observed number of heads will not deviate from $n/2$ by more than $100$ with a probability of at least $0.99$, provided that $n$ is sufficiently large?
\begin{answer}
No. Let $X_i=1$ if a head occurs on the $i$th toss and $X_i=0$ otherwise, and let $S_n = \sum_{i=1}^n X_i$ be the total number of heads after $n$ tosses. Because the coin is fair, $\expe(S_n)=n/2$ and $\var(S_n)=n/4$. Applying Chebyshev's inequality to $S_n$ with $\epsilon=100$:
\[
\prob(|S_n - n/2| > 100) \leq \var(S_n)/100^2
\]
But $\var(S_n)\to\infty$ as $n\to\infty$, so the probability that $S_n$ deviates from $n/2$ by a fixed amount cannot be bounded using Chebyshev's inequality.
\end{answer}


\question
Let $X$ be a random variable with mean $\mu\neq 0$ and variance $\sigma^2$. The \emph{relative deviation} of $X$ from its mean is defined by $\displaystyle D  = \left|\frac{X-\mu}{\mu}\right|$. Use Chebyshev's inequality to show that 
$
\displaystyle \prob(D \geq \epsilon) \leq \left(\frac{\sigma}{\mu\epsilon}\right)^2.
$
\begin{answer} 
By Chebyshev's inequality,
\[
\prob(D\geq \epsilon) 
	= \prob\left(\left|\frac{X-\mu}{\mu}\right|\geq \epsilon\right)
	= \prob\big(|X-\mu|\geq |\mu|\epsilon)
	\leq \frac{\sigma^2}{\mu^2 \epsilon^2}
\]
\end{answer}

\question
\begin{parts}
\part\label{qu:pt:cheb_bern}
Let $X_1,X_2,\ldots,X_n$ be independent with each $X_i\sim\text{Bernoulli}(p)$. Using the fact that $p(1-p)\leq 1/4$ for all $0<p<1$, show that for any $\epsilon > 0$, 
\[
\prob\left(\left|\frac{1}{n}\sum_{i=1}^n X_i - p\right|\geq\epsilon\right) \leq \frac{1}{4n\epsilon^2}.
\]
\begin{answer} % <<<
For the $\text{Binomial}(n,p)$ distribution, Chebyshev's inequality yields
\[
\prob\left(\left|\frac{S_n}{n}-p\right|\geq\epsilon\right) \leq \frac{p(1-p)}{n\epsilon^2}
\]
and the result follows because $p(1-p)\leq 1/4$.
\end{answer}
\part
Let $A$ be an event associated with a random experiment. Suppose that the experiment is repeated $n$ times. Let $X_i$ be the indicator variable of the event that $A$ occurs during the $i$th trial. Then $X_i\sim\text{Bernoulli}(p)$ where $p=\prob(A)$, and the sample mean $\frac{1}{n}\sum_{i=1}^n X_i$ is the \emph{relative frequency} of $A$ over these $n$ trials. What does the upper bound derived in part \ref{qu:pt:cheb_bern} say about the relative frequency of event $A$ as $n\to\infty$.
\begin{answer} % <<<
The probability that the relative frequency $\displaystyle\frac{1}{n}\sum_{i=1}^n X_i$ of $A$ differs from its true probability $p=\prob(A)$ by more than a fixed amount (however small), tends to zero as the number of trials increases to infinity.
\end{answer}
\end{parts}


\end{questions}
\end{exercise}
