% !TEX root = main.tex

%-------------------------------------------------
\section{Moments}\label{sec:moments}

%-----------------------------
\subsection{Expectation of transformed variables}\label{sec:expe_transformed}

Let $X$ be a random variable and let $g:\R\to\R$ be a transformation. 

\ben
\it If $g$ is a \emph{non-negative} function, the expected value of $g(X)$ is
\[
\expe\big[g(X)\big] = \begin{cases}
	\displaystyle\sum_{i=1}^{\infty} g(x_i)f(x_i)		& \text{if $g(X)$ is discrete, and} \\[3ex]
	\displaystyle\int_{-\infty}^{\infty} g(x)f(x)\,dx	& \text{if $g(X)$ is continuous.} 
\end{cases}
\]

\it If $g$ is a \emph{signed function}, the expected value of $g(X)$ is
\[
\expe\big[g(X)\big] = \expe\big[g^{+}(X)\big] - \expe\big[g^{-}(X)\big],
\]
provided $\expe\big[g^{+}(X)\big]$ and $\expe\big[g^{-}(X)\big]$ are not both infinite, where $g^{+}$ and $g^{-}$ are respectively the positive and negative parts of $g$, 
\[
g^{+}(x)	= \begin{cases}  g(x) & \text{ if } g(x)\geq 0, \\ 0 & \text{otherwise,}\end{cases}
\qquad\text{and}\qquad
g^{-}(x)	= \begin{cases} -g(x) & \text{ if } g(x)<0, \\ 0 & \text{otherwise.}\end{cases}
\]
\een

\begin{example}
For $X\sim\text{Uniform}[-1,1]$ find $\expe(1/X^2)$ and $\expe(1/X)$.
\begin{solution}
The PDF of $X$ is $f(x)=1/2$ for $x\in[-1,1]$ and zero otherwise.
\ben
\it Let $g(x) = 1/x^2$. This is a non-negative function, so
\begin{align*}
\expe\left(\frac{1}{X^2}\right) 
	= \int_{-\infty}^{\infty} g(x)f(x)\,dx 
	= \frac{1}{2}\int_{-1}^{1} \frac{1}{x^2}\,dx
	= \int_{0}^{1}\frac{1}{x^2}\,dx
	= \infty,
\end{align*}
%so $\expe(1/X^2)$ is infinite.

\it Let $g(x) = 1/x$. This is a signed function, so we must consider its positive and negative parts separately:
\[
g^{+}(x)	= \begin{cases}  1/x & \text{ if}\quad x\geq 0, \\ 0 & \text{ if}\quad x < 0,\end{cases}
\qquad\text{and}\qquad
g^{-}(x)	= \begin{cases}  0 & \text{ if}\quad x\geq 0, \\ -1/x & \text{ if}\quad x < 0.\end{cases}
\]
Thus
\begin{align*}
\expe\left(\frac{1}{X}\right)
	& = \int_{0}^{\infty} g^{+}(x)f(x)\,dx - \int_{0}^{\infty} g^{-}(x) f(x)\,dx \\
	& = \frac{1}{2}\int_0^1 \frac{1}{x}\,dx - \frac{1}{2}\int_{-1}^0 \frac{-1}{x}\,dx \\
	& = \frac{1}{2}\int_0^1 \frac{1}{x}\,dx - \frac{1}{2}\int_{0}^{1} \frac{1}{x}\,dx \\
	& = \infty - \infty,
\end{align*}
so $\expe(1/X)$ is undefined.
\een
\end{solution}
\end{example}

%-----------------------------
\subsection{Moments}

We now consider transformations of the form $g(x)=x^k$ for various values of $k\in\Z$.
% definition: raw moments
\begin{definition}
The expectation $\expe(X^{k})$ is called the \emph{$k$th moment about the origin} of $X$.
\end{definition}

%In particular,
\bit
\it $\expe(X^0) = 1$,
\it $\expe(X)$ is the \emph{mean} of $X$, often denoted by $\mu$,
\it $\expe(X^2)$ is the \emph{mean-square} of $X$.
\eit

% definition: central moments
\begin{definition}
The expectation $\expe\big[(X-\mu)^{k}\big]$ is called the \emph{$k$th moment about the mean} of $X$.
\end{definition}

%In particular,
\bit
\it $\expe\big[(X-\mu)^{0}\big] = 1$,
\it $\expe\big[(X-\mu)\big] = 0$,
\it $\expe\big[(X-\mu)^2\big]$ is the \emph{variance} of $X$, often denoted by $\sigma^2$.
\it $\expe\big[(X-\mu)^2\big]^{1/2}$ is the \emph{standard deviation} of $X$, often denoted by $\sigma$.
%\it $\sqrt{\mu_2}$ is the \emph{standard deviation} of $X$, denoted by $\sigma$.
\eit


%-----------------------------
\subsection{Location, scale and shape} 

When trying to describe a distribution, it is natural to look for its \emph{location}, \emph{scale} (size) and \emph{shape}.

%% mean and variance
%For any random variable $X:\Omega\to\R$,
%\ben
%\it the first moment of $X$ about the origin ($\mu$)  describes its location, 
%\it the second moment of $X$ about the mean ($\sigma^2$) describes its scale, and
%\it the higher moments of $X$ describe the shape of its distribution.
%\een

\paragraph{Location}
To locate $X$, we look for a point $c\in\R$ such that the expected squared deviation $\expe\big[(X-c)^2\big]$ around this point is minimum. By the linearity of expectation,
\[
\expe\big[(X-c)^2\big] = \expe(X^2 - 2cX + c^2) = \expe(X^2) - 2c\expe(X) + c^2
\]
To find the value of $c$ that minimises the expected squared deviation, we differentiate the right-hand side with respect to $c$ and set the resulting expression to zero. This yields $c=\expe(X)$, so the location of $X$ is described by its \emph{expectation} (or first moment about the origin), $\mu$. 

\paragraph{Scale}
The size of a distribution should not depend on its location, so we consider the \emph{centred} variable 
\[
Y=X-\expe(X),
\]
which has the property $\expe(Y)=0$. The expected squared deviation of $X$ around its mean $\expe(X)$ is its \emph{variance} (or second moment about the mean), so the size of $X$ is described by its \emph{standard deviation}, $\sigma$.

\paragraph{Shape}
The shape of a distribution should not depend on its location nor its scale. Thus we consider the higher moments of the so-called \emph{standardised} variable,
\[
Z = \frac{X-\mu}{\sigma},
\]
which has the properties $\expe(Z)=0$ and $\var(Z)=1$.

% definition
\begin{definition}
The \emph{skewness} of a random variable $X$ is defined to be
\[
\gamma_1 = \expe\left[\left(\frac{X-\mu}{\sigma}\right)^3\right] %= \frac{\mu_{3}}{\sigma^3}
\]
%where $\mu_3$ is the third central moment of $X$, and $\sigma$ is its standard deviation. 
\end{definition}

Skewness is a measure of \emph{asymmetry}:
\bit
\it Negative skew ($\gamma_1 < 0$): long tail on the left, mass concentrated on the right.
\it Positive skew ($\gamma_1 > 0$): long tail on the right, mass concentrated on the left.
\eit

% example: skewness of binomial
\begin{example}\label{ex:skewness_binomial}
If $X\sim\text{Binomial}(n,p)$, some tedious algebra shows that the skewness of $X$ is 
\[
\gamma_1 = \frac{1-2p}{\sqrt{np(1-p)}} = \begin{cases}
	< 0 	& \text{if } p > \frac{1}{2} \\
	= 0 	& \text{if } p = \frac{1}{2} \\
	> 0 	& \text{if } p < \frac{1}{2} \\
\end{cases}	
\]
\end{example}

% definition: kurtosis
\begin{definition}
The \emph{excess kurtosis} of a random variable $X$ is defined to be
\[
\gamma_2 = \expe\left[\left(\frac{X-\mu}{\sigma}\right)^4\right] -3.%= \frac{\mu_{4}}{\sigma^4},
\]
%and the \emph{excess kurtosis} is $\gamma_2 = \beta_2 - 3$.
\end{definition}

Kurtosis is a measure of \emph{peakiness}. The normal distribution has $\gamma_2=0$, so excess kurtosis provides a measure of peakiness relative to that of the normal distribution.
\bit
\it Negative excess kurtosis ($\gamma_2 < 0$): tall and peaky with heavy tails (\emph{leptokurtic}),
\it Positive excess kurtosis ($\gamma_2 > 0$): low and wide with light tails (\emph{platykurtic}).
\eit

\begin{example}\label{ex:kurtosis_binomial}
If $X\sim\text{Binomial}(n,p)$ some more tedious algebra shows that the excess kurtosis of $X$ is
\[
\gamma_2 = \frac{1-6p(1-p)}{np(1-p)} = \begin{cases}
	< 0 				& \text{if } \left|p -\frac{1}{2}\right| < \frac{1}{2\sqrt{3}} \\
	> 0 				& \text{if } \left|p -\frac{1}{2}\right| > \frac{1}{2\sqrt{3}} \\
\end{cases}
\]
\end{example}

% exercises: moments
\begin{exercise}
\begin{questions}

\question
Let $X$ be a continuous random variable with uniform density on the interval $[-1,1]$,
\[
f(x) = \begin{cases}
	\frac{1}{2}	& \text{ if } x\in[-1,+1] \\
	0			& \text{ otherwise.}
\end{cases}
\]
Compute the moments $\expe(X)$, $\expe(X^2)$, $\expe(X^3)$, $\expe(1/X)$ and $\expe(1/X^2)$.
\begin{answer}
Since $xf(x)$, $x^2f(x)$ and $x^3f(x)$ are continuous and bounded over $\supp(f)=[-1,1]$, the first three expectations can be computed using definite integrals. This is not the case for$\expe(1/X)$ and $\expe(1/X^2)$.

\ben
\it % << (i)
$\displaystyle\expe(X) = \frac{1}{2}\int_{-1}^1 x\,dx = 0.$
%Let $g(x) = x$. This is a signed function, so
%\begin{align*}
%\expe(X) 
%	& = \int_{-\infty}^{\infty} g^{+}(x)f(x)\,dx - \int_{-\infty}^{\infty} g^{-}(x) f(x)\,dx \\
%	& = \frac{1}{2}\int_0^1 x\,dx - \frac{1}{2}\int_{-1}^0 (-x)\,dx \\
%	& = \frac{1}{2}\left[\frac{x^2}{2}\right]_0^1 - \frac{1}{2}\left[\frac{-x^2}{2}\right]_{-1}^0
%	= \left(\frac{1}{4}-0\right) - \left(0 + \frac{1}{4}\right)
%	= 0.
%\end{align*}
%
\it % << (ii)
$\displaystyle\expe(X^2) = \frac{1}{2}\int_{-1}^1 x^2\,dx = \frac{1}{3}.$
%Let $g(x) = x^2$. This is a non-negative function, so
%\begin{align*}
%\expe(X^2) 
%	& = \int_{-\infty}^{\infty} g(x)f(x)\,dx \\
%	& = \frac{1}{2}\int_{-1}^{1} x^2\,dx
%	= \int_{0}^{1} x^2\,dx
%	= \left[\frac{x^3}{3}\right]_0^1
%	= \frac{1}{3}.
%\end{align*}
%
\it % << (iii)
$\displaystyle\expe(X^3) = \frac{1}{2}\int_{-1}^1 x^3\,dx = 0.$
%Let $g(x) = x^3$. This is a signed function, so
%\begin{align*}
%\expe(X^3) 
%	& = \int_{-\infty}^{\infty} g^{+}(x)f(x)\,dx - \int_{-\infty}^{\infty} g^{-}(x) f(x)\,dx \\
%	& = \frac{1}{2}\int_0^1 x^3\,dx - \frac{1}{2}\int_{-1}^0 (-x^3)\,dx \\
%	& = \frac{1}{2}\left[\frac{x^4}{4}\right]_0^1 - \frac{1}{2}\left[\frac{-x^4}{4}\right]_{-1}^0 
%	= \left(\frac{1}{8}-0\right) - \left(0 + \frac{1}{8}\right) 
%	= 0.
%\end{align*}

\it % << (iv)
Let $g(x) = 1/x$. This is a signed function, so
\begin{align*}
\expe\left(\frac{1}{X}\right)
	& = \int_{-\infty}^{\infty} g^{+}(x)f(x)\,dx - \int_{-\infty}^{\infty} g^{-}(x) f(x)\,dx \\
	& = \frac{1}{2}\int_0^1 \frac{1}{x}\,dx - \frac{1}{2}\int_{-1}^0 \frac{-1}{x}\,dx \\
	& = \frac{1}{2}\int_0^1 \frac{1}{x}\,dx - \frac{1}{2}\int_{0}^{1} \frac{1}{x}\,dx 
	= \infty - \infty
\end{align*}
so $\expe(1/X)$ is undefined.

\it % << (v)
Let $g(x) = 1/x^2$. This is a non-negative function, so
\begin{align*}
\expe\left(\frac{1}{X^2}\right)
	& = \int_{-\infty}^{\infty} g(x)f(x)\,dx \\
	& = \frac{1}{2}\int_{-1}^{1} \frac{1}{x^2}\,dx
	= \int_{0}^{1}\frac{1}{x^2}\,dx
	= \infty
\end{align*}
so $\expe(1/X^2)$ is infinite.
\een
\end{answer} 

\question
Let $X$ be a continuous random variable with the following PDF:
\[
f(x) = \begin{cases}
	1-|x|	& \text{ if } x\in[-1,1] \\
	0		& \text{ otherwise.}
\end{cases}
\]
For what values of $k\in\Z$ do the moments $\expe(X^{k})$ exist?  
\begin{answer}
For $k>0$,
\[
\expe(X^{k}) = \int_{-1}^{0}x^{k}(1+x)\,dx + \int_{0}^{1}x^{k}(1-x)\,dx < \infty
\]
Let $k < 0$. If $k$ is even then $X^k$ is non-negative, so
\begin{align*}
\expe(X^k)
	& = \expe\big((X^{+})^{k}\big) = +\infty\\
\intertext{If $k$ is odd,}
\expe(X^k)
	& = \expe\big((X^{+})^{k}\big) - \expe\big((X^{-})^{k}\big) 
		= \infty - \infty 
\end{align*}
so in this case the moment $\expe(X^k)$ does not exist.
\end{answer} 

\question
Let $X$ be a discrete random variable with the following PMF:
\[
\prob(X=x) = \frac{45}{\pi^4x^4} \qquad\text{for $x=\pm 1,\pm 2,\pm 3\ldots$ (and zero otherwise).}
\]
\begin{parts}
\part For what values of $k\in\Z$ do the moments $\expe(X^{k})$ exist?  
\begin{answer}
Let $c = 45/\pi^4$.
\bit
\it If $k\leq 2$ then $\expe(X^k)$ exists and is finite because 
\[
\expe(X^k) 
	= c\sum_{x\neq 0}\frac{x^k}{x^4} 
	= 2c\sum_{x=1}^{\infty}\frac{1}{x^{4-k}}
	< \infty.
\]
\it If $k>2$ and $k$ is even, $\expe(X^k)$ is infinite because
\[
\expe(X^k) 
	= c\sum_{x\neq 0}\frac{x^k}{x^4} 
	= c\sum_{x=1}^{\infty}\frac{1}{x^{4-k}} 
	= \infty.
\]
If $k>2$ and $k$ is odd, $\expe(X^k)$ does not exist because
\[
\expe(X^k) 
	= c\sum_{x\neq 0}\frac{x^k}{x^4} 
	= c\sum_{x=1}^{\infty}\frac{1}{x^{4-k}} - c\sum_{x=1}^{\infty}\frac{1}{x^{4-k}} 
	= \infty - \infty.
\]
\eit
\end{answer}
\part Compute the variance of $X$ and its first negative moment $\expe(X^{-1})$.
\begin{answer}
The first two moments of $X$ are	
\begin{align*}
\expe(X) 
	& = c\sum_{x\neq 0}\frac{1}{x^3} 
	  = c\sum_{x=1}^{\infty}\frac{1}{x^3} - c\sum_{x=1}^{\infty}\frac{1}{x^3} = 0 \\
\expe(X^2)
	& = c\sum_{x\neq 0}\frac{1}{x^2} 
	  = 2c\sum_{x=1}^{\infty}\frac{1}{x^2}
	  = 2\left(\frac{45}{\pi^4}\right)\left(\frac{\pi^2}{6}\right) = \frac{15}{\pi^2},
\end{align*}
so $\var(X)=15/\pi^2$. Similarly,
\begin{align*}
\expe\left(\frac{1}{X}\right) 
	& = c\sum_{x\neq 0}^{\infty}\frac{1}{x^5} \\
	& = c\sum_{n=1}^{\infty}\frac{1}{n^5} - c\sum_{x=1}^{\infty}\frac{1}{x^5} \\
	& = 0.
\end{align*}
In fact, all negative odd-integer moments are zero.
\end{answer}
\end{parts}

\end{questions}
\end{exercise}

